{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"kobart_summarization.ipynb","provenance":[],"collapsed_sections":["IDNtTnpcTCLH"],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyPUZDa0tK8ic7XNZwy12/Nd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"k83B1GLSScs_"},"source":["## 환경설정 확인(GPU, RAM)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jCpDp4Mch0oS","executionInfo":{"status":"ok","timestamp":1635062277937,"user_tz":-540,"elapsed":39,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"}},"outputId":"0bc1ae3b-076d-45f2-a202-4f31b05be2e3"},"source":["!cat /proc/cpuinfo"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["processor\t: 0\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n","stepping\t: 3\n","microcode\t: 0x1\n","cpu MHz\t\t: 2000.154\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 8\n","core id\t\t: 0\n","cpu cores\t: 4\n","apicid\t\t: 0\n","initial apicid\t: 0\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n","bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n","bogomips\t: 4000.30\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 1\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n","stepping\t: 3\n","microcode\t: 0x1\n","cpu MHz\t\t: 2000.154\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 8\n","core id\t\t: 1\n","cpu cores\t: 4\n","apicid\t\t: 2\n","initial apicid\t: 2\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n","bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n","bogomips\t: 4000.30\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 2\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n","stepping\t: 3\n","microcode\t: 0x1\n","cpu MHz\t\t: 2000.154\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 8\n","core id\t\t: 2\n","cpu cores\t: 4\n","apicid\t\t: 4\n","initial apicid\t: 4\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n","bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n","bogomips\t: 4000.30\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 3\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n","stepping\t: 3\n","microcode\t: 0x1\n","cpu MHz\t\t: 2000.154\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 8\n","core id\t\t: 3\n","cpu cores\t: 4\n","apicid\t\t: 6\n","initial apicid\t: 6\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n","bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n","bogomips\t: 4000.30\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 4\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n","stepping\t: 3\n","microcode\t: 0x1\n","cpu MHz\t\t: 2000.154\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 8\n","core id\t\t: 0\n","cpu cores\t: 4\n","apicid\t\t: 1\n","initial apicid\t: 1\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n","bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n","bogomips\t: 4000.30\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 5\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n","stepping\t: 3\n","microcode\t: 0x1\n","cpu MHz\t\t: 2000.154\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 8\n","core id\t\t: 1\n","cpu cores\t: 4\n","apicid\t\t: 3\n","initial apicid\t: 3\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n","bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n","bogomips\t: 4000.30\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 6\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n","stepping\t: 3\n","microcode\t: 0x1\n","cpu MHz\t\t: 2000.154\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 8\n","core id\t\t: 2\n","cpu cores\t: 4\n","apicid\t\t: 5\n","initial apicid\t: 5\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n","bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n","bogomips\t: 4000.30\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 7\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 85\n","model name\t: Intel(R) Xeon(R) CPU @ 2.00GHz\n","stepping\t: 3\n","microcode\t: 0x1\n","cpu MHz\t\t: 2000.154\n","cache size\t: 39424 KB\n","physical id\t: 0\n","siblings\t: 8\n","core id\t\t: 3\n","cpu cores\t: 4\n","apicid\t\t: 7\n","initial apicid\t: 7\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n","bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n","bogomips\t: 4000.30\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mwfSqHpjL70z","executionInfo":{"status":"ok","timestamp":1635062277939,"user_tz":-540,"elapsed":26,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"}},"outputId":"78d64892-1904-4f5d-cd6c-adda4ec5d2d7"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Oct 24 07:57:55 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","metadata":{"id":"auA8uwl7N0wa"},"source":["## 7. 생성 요약 모델링"]},{"cell_type":"markdown","metadata":{"id":"OmI20LXlN52W"},"source":["### 1. 문장 생성 요약 모델 (koBART 모델)\n","> 참고자료 링크 : https://github.com/ChoiSaeyoun/kobart_summarization  \n","> 사용 데이터셋 링크 : https://aihub.or.kr/aidata/30714  \n","> streamlit 참고자료 : https://developer-ping9.tistory.com/115"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJMJ5yoRm5zK","executionInfo":{"status":"ok","timestamp":1635062301884,"user_tz":-540,"elapsed":19053,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"}},"outputId":"ff34b97e-7099-4a35-eead-05154721b2dd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BP_HcduXm5B8","executionInfo":{"status":"ok","timestamp":1635062301886,"user_tz":-540,"elapsed":18,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"}},"outputId":"cecc66ba-be4d-41c8-ee72-7ad788eb79ce"},"source":["cd '/content/drive/MyDrive/munning_rachine/KoBART'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/munning_rachine/KoBART\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6cX3yiL9N5Ji","executionInfo":{"elapsed":2090,"status":"ok","timestamp":1631437236448,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"33d3a866-b827-47b5-e81a-ed04c77ccab7"},"source":["# !git clone https://github.com/seujung/KoBART-summarization.git\n","# mv KoBART-summarization KoBART"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'KoBART-summarization'...\n","remote: Enumerating objects: 104, done.\u001b[K\n","remote: Counting objects: 100% (104/104), done.\u001b[K\n","remote: Compressing objects: 100% (78/78), done.\u001b[K\n","remote: Total 104 (delta 52), reused 66 (delta 23), pack-reused 0\u001b[K\n","Receiving objects: 100% (104/104), 37.22 MiB | 32.19 MiB/s, done.\n","Resolving deltas: 100% (52/52), done.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"JeBxTk6vKH2l","executionInfo":{"elapsed":417,"status":"ok","timestamp":1632646018044,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"c87a8beb-6c6f-4564-ffb6-f836991a7b25"},"source":["pwd"],"execution_count":null,"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/munning_rachine/KoBART'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"_pkFQhs-ma6y"},"source":["#### 라이브러리 설치"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"YKsCkvkQmGL6","executionInfo":{"status":"ok","timestamp":1635062542310,"user_tz":-540,"elapsed":239914,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"}},"outputId":"835495d1-c009-43f8-ac52-f7d39fd350c5"},"source":["!pip install git+https://github.com/SKT-AI/KoBART#egg=kobart\n","!pip install -r requirements.txt\n","!pip install gdown\n","!pip install rouge\n","# !pip install pyngrok #밑에 동일한 코드 있음\n","!pip install torchtext==0.8.0 torch==1.7.1 pytorch-lightning==1.2.2"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kobart\n","  Cloning https://github.com/SKT-AI/KoBART to /tmp/pip-install-hnxuph8r/kobart_e6b6582a386b496fa39e4d85e772b801\n","  Running command git clone -q https://github.com/SKT-AI/KoBART /tmp/pip-install-hnxuph8r/kobart_e6b6582a386b496fa39e4d85e772b801\n","Collecting transformers==4.3.3\n","  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 14.5 MB/s \n","\u001b[?25hCollecting torch==1.7.1\n","  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n","\u001b[K     |████████████████████████████████| 776.8 MB 15 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->kobart) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->kobart) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (4.8.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (4.62.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (3.3.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 53.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->kobart) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 60.2 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.3->kobart) (3.6.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.3.3->kobart) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.3->kobart) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3->kobart) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3->kobart) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.3->kobart) (1.0.1)\n","Building wheels for collected packages: kobart\n","  Building wheel for kobart (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobart: filename=kobart-0.4-py3-none-any.whl size=8538 sha256=ff47bcd053c140488596add089b03ddc9e0d74340bda9c5f8ae75b6b83d56712\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-brvmnycg/wheels/6e/55/c4/bd4fede223bc304089ac8da2a2099a69db3fcd4b0e853383f5\n","Successfully built kobart\n","Installing collected packages: tokenizers, sacremoses, transformers, torch, kobart\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.9.0+cu111\n","    Uninstalling torch-1.9.0+cu111:\n","      Successfully uninstalled torch-1.9.0+cu111\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.10.0+cu111 requires torch==1.9.0, but you have torch 1.7.1 which is incompatible.\n","torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n","Successfully installed kobart-0.4 sacremoses-0.0.46 tokenizers-0.10.3 torch-1.7.1 transformers-4.3.3\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.1.5)\n","Collecting torch==1.9.0\n","  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[K     |████████████████████████████████| 831.4 MB 2.3 kB/s \n","\u001b[?25hCollecting transformers==4.8.2\n","  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 54.5 MB/s \n","\u001b[?25hCollecting pytorch-lightning==1.3.8\n","  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\n","\u001b[K     |████████████████████████████████| 813 kB 80.1 MB/s \n","\u001b[?25hCollecting streamlit==0.72.0\n","  Downloading streamlit-0.72.0-py2.py3-none-any.whl (7.4 MB)\n","\u001b[K     |████████████████████████████████| 7.4 MB 81.6 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->-r requirements.txt (line 2)) (3.7.4.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (3.13)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (4.8.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (0.0.46)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (21.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (0.10.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (4.62.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (3.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2->-r requirements.txt (line 3)) (2019.12.20)\n","Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n","  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 83.7 MB/s \n","\u001b[?25hCollecting pyyaml\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 81.2 MB/s \n","\u001b[?25hCollecting future>=0.17.1\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 75.0 MB/s \n","\u001b[?25hCollecting pyDeprecate==0.3.0\n","  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (2.6.0)\n","Collecting torchmetrics>=0.2.0\n","  Downloading torchmetrics-0.5.1-py3-none-any.whl (282 kB)\n","\u001b[K     |████████████████████████████████| 282 kB 72.8 MB/s \n","\u001b[?25hRequirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (7.1.2)\n","Collecting gitpython\n","  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n","\u001b[K     |████████████████████████████████| 180 kB 91.8 MB/s \n","\u001b[?25hCollecting validators\n","  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (2.8.2)\n","Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (0.8.1)\n","Collecting pydeck>=0.1.dev5\n","  Downloading pydeck-0.7.0-py2.py3-none-any.whl (4.3 MB)\n","\u001b[K     |████████████████████████████████| 4.3 MB 70.2 MB/s \n","\u001b[?25hRequirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (5.1.1)\n","Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (4.2.4)\n","Collecting base58\n","  Downloading base58-2.1.0-py3-none-any.whl (5.6 kB)\n","Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (3.17.3)\n","Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (4.1.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (0.10.2)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (7.1.2)\n","Collecting blinker\n","  Downloading blinker-1.4.tar.gz (111 kB)\n","\u001b[K     |████████████████████████████████| 111 kB 92.2 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (3.0.0)\n","Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit==0.72.0->-r requirements.txt (line 5)) (1.5.1)\n","Collecting watchdog\n","  Downloading watchdog-2.1.6-py3-none-manylinux2014_x86_64.whl (76 kB)\n","\u001b[K     |████████████████████████████████| 76 kB 5.7 MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2018.9)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.72.0->-r requirements.txt (line 5)) (2.6.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.72.0->-r requirements.txt (line 5)) (0.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.72.0->-r requirements.txt (line 5)) (2.11.3)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==0.72.0->-r requirements.txt (line 5)) (0.11.1)\n","Collecting aiohttp\n","  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 66.4 MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.8.2->-r requirements.txt (line 3)) (2.4.7)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit==0.72.0->-r requirements.txt (line 5)) (1.15.0)\n","Collecting ipykernel>=5.1.2\n","  Downloading ipykernel-6.4.2-py3-none-any.whl (124 kB)\n","\u001b[K     |████████████████████████████████| 124 kB 91.1 MB/s \n","\u001b[?25hRequirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (7.6.5)\n","Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (5.1.0)\n","Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (5.3.5)\n","Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.1.3)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.2.0)\n","Requirement already satisfied: argcomplete>=1.12.3 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (1.12.3)\n","Collecting ipython<8.0,>=7.23.1\n","  Downloading ipython-7.28.0-py3-none-any.whl (788 kB)\n","\u001b[K     |████████████████████████████████| 788 kB 76.9 MB/s \n","\u001b[?25hRequirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (1.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.8.2->-r requirements.txt (line 3)) (3.6.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (2.6.1)\n","Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n","  Downloading prompt_toolkit-3.0.21-py3-none-any.whl (374 kB)\n","\u001b[K     |████████████████████████████████| 374 kB 76.9 MB/s \n","\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.2.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (57.4.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.7.5)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (4.8.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.18.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (5.1.3)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (3.5.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (1.0.2)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.8.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit==0.72.0->-r requirements.txt (line 5)) (2.0.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (22.3.0)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (4.8.1)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.2.5)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.41.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.37.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.6.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.12.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (3.3.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.4.6)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.35.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (1.3.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 3)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 3)) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 3)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.8.2->-r requirements.txt (line 3)) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (3.1.1)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (5.3.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (1.8.0)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.12.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (5.6.1)\n","Collecting async-timeout<4.0,>=3.0\n","  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 85.3 MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 85.5 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r requirements.txt (line 4)) (21.2.0)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.8-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (4.1.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (1.5.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.7.1)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.5.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.8.4)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==0.72.0->-r requirements.txt (line 5)) (0.5.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.2->-r requirements.txt (line 3)) (1.0.1)\n","Building wheels for collected packages: future, blinker\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=215313a2e648105f6917b9860c4ee0162975a8594436133a02edbcef0d388a72\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=fd2b463ea44ea6df556050e4ce6695e561e1126730ff46132fa32c96332df42b\n","  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n","Successfully built future blinker\n","Installing collected packages: prompt-toolkit, ipython, ipykernel, multidict, yarl, smmap, async-timeout, torch, gitdb, fsspec, aiohttp, watchdog, validators, torchmetrics, pyyaml, pyDeprecate, pydeck, huggingface-hub, gitpython, future, blinker, base58, transformers, streamlit, pytorch-lightning\n","  Attempting uninstall: prompt-toolkit\n","    Found existing installation: prompt-toolkit 1.0.18\n","    Uninstalling prompt-toolkit-1.0.18:\n","      Successfully uninstalled prompt-toolkit-1.0.18\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 5.5.0\n","    Uninstalling ipython-5.5.0:\n","      Successfully uninstalled ipython-5.5.0\n","  Attempting uninstall: ipykernel\n","    Found existing installation: ipykernel 4.10.1\n","    Uninstalling ipykernel-4.10.1:\n","      Successfully uninstalled ipykernel-4.10.1\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.7.1\n","    Uninstalling torch-1.7.1:\n","      Successfully uninstalled torch-1.7.1\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.3.3\n","    Uninstalling transformers-4.3.3:\n","      Successfully uninstalled transformers-4.3.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","kobart 0.4 requires torch==1.7.1, but you have torch 1.9.0 which is incompatible.\n","kobart 0.4 requires transformers==4.3.3, but you have transformers 4.8.2 which is incompatible.\n","jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.21 which is incompatible.\n","google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.4.2 which is incompatible.\n","google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.28.0 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 base58-2.1.0 blinker-1.4 fsspec-2021.10.1 future-0.18.2 gitdb-4.0.8 gitpython-3.1.24 huggingface-hub-0.0.12 ipykernel-6.4.2 ipython-7.28.0 multidict-5.2.0 prompt-toolkit-3.0.21 pyDeprecate-0.3.0 pydeck-0.7.0 pytorch-lightning-1.3.8 pyyaml-5.4.1 smmap-5.0.0 streamlit-0.72.0 torch-1.9.0 torchmetrics-0.5.1 transformers-4.8.2 validators-0.18.2 watchdog-2.1.6 yarl-1.7.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","ipykernel","prompt_toolkit"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n","Collecting rouge\n","  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n","Installing collected packages: rouge\n","Successfully installed rouge-1.0.1\n","Collecting torchtext==0.8.0\n","  Downloading torchtext-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (6.9 MB)\n","\u001b[K     |████████████████████████████████| 6.9 MB 10.8 MB/s \n","\u001b[?25hCollecting torch==1.7.1\n","  Using cached torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n","Collecting pytorch-lightning==1.2.2\n","  Downloading pytorch_lightning-1.2.2-py3-none-any.whl (816 kB)\n","\u001b[K     |████████████████████████████████| 816 kB 65.9 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (4.62.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (3.7.4.3)\n","Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.2) (0.18.2)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.2) (2.6.0)\n","Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.2) (2021.10.1)\n","Collecting PyYAML!=5.4.*,>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 75.1 MB/s \n","\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.2.2) (3.7.4.post0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (1.8.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (0.12.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (0.6.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (57.4.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (3.3.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (1.35.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (3.17.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (0.37.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (0.4.6)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (1.41.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (1.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (4.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (3.1.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2) (1.7.0)\n","Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2) (3.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2) (21.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2) (5.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (3.6.0)\n","Installing collected packages: torch, PyYAML, torchtext, pytorch-lightning\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.9.0\n","    Uninstalling torch-1.9.0:\n","      Successfully uninstalled torch-1.9.0\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 5.4.1\n","    Uninstalling PyYAML-5.4.1:\n","      Successfully uninstalled PyYAML-5.4.1\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.10.0\n","    Uninstalling torchtext-0.10.0:\n","      Successfully uninstalled torchtext-0.10.0\n","  Attempting uninstall: pytorch-lightning\n","    Found existing installation: pytorch-lightning 1.3.8\n","    Uninstalling pytorch-lightning-1.3.8:\n","      Successfully uninstalled pytorch-lightning-1.3.8\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.10.0+cu111 requires torch==1.9.0, but you have torch 1.7.1 which is incompatible.\n","kobart 0.4 requires transformers==4.3.3, but you have transformers 4.8.2 which is incompatible.\u001b[0m\n","Successfully installed PyYAML-6.0 pytorch-lightning-1.2.2 torch-1.7.1 torchtext-0.8.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"HjOOwASitYzW"},"source":["### 일상대화 데이터 전처리 하기"]},{"cell_type":"code","metadata":{"id":"uNWtx_V-mqSH"},"source":["import pandas as pd\n","import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6dhto4d1tbHp"},"source":["json_path = '/content/drive/MyDrive/munning_rachine/dataset/korean_dialogue/Training/json/'\n","csv_path = '/content/drive/MyDrive/munning_rachine/dataset/korean_dialogue/Training/csv/'\n","\n","category = ['개인및관계', '미용과건강', '상거래(쇼핑)', '시사교육', '식음료', '여가생활', '일과직업', '주거와생활', '행사']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7n45rpFx4qW","executionInfo":{"elapsed":769180,"status":"ok","timestamp":1633244080685,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"dc6c293b-8e39-4fb6-c0a0-9ec4e64ae52e"},"source":["for file in category:\n","    print('file 명: ', file, '.json') \n","    with open(json_path + file+'.json') as f:\n","        json_data = json.load(f)\n","        \n","        df = pd.DataFrame(json_data)\n","        df = df['data']\n","        \n","        new_df = pd.DataFrame(columns=['text', 'abstractive'])\n","        \n","        for i in range(len(df)):\n","            new = pd.DataFrame(df[i]['body']['dialogue'])\n","            text = ' '.join(list(new['utterance']))\n","            abstractive = df[i]['body']['summary']\n","\n","            new_df.loc[i, 'text'] = text\n","            new_df.loc[i, 'abstractive'] = abstractive\n","\n","        print('컬럼 null 개수는')    \n","        print(new_df.isnull().sum())\n","          \n","        new_df.to_csv(csv_path + file+'.csv')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["file 명:  개인및관계 .json\n","text           0\n","abstractive    0\n","dtype: int64\n","file 명:  미용과건강 .json\n","text           0\n","abstractive    0\n","dtype: int64\n","file 명:  상거래(쇼핑) .json\n","text           0\n","abstractive    0\n","dtype: int64\n","file 명:  시사교육 .json\n","text           0\n","abstractive    0\n","dtype: int64\n","file 명:  식음료 .json\n","text           0\n","abstractive    0\n","dtype: int64\n","file 명:  여가생활 .json\n","text           0\n","abstractive    0\n","dtype: int64\n","file 명:  일과직업 .json\n","text           0\n","abstractive    0\n","dtype: int64\n","file 명:  주거와생활 .json\n","text           0\n","abstractive    0\n","dtype: int64\n","file 명:  행사 .json\n","text           0\n","abstractive    0\n","dtype: int64\n"]}]},{"cell_type":"code","metadata":{"id":"AFK3GcXYxv2g"},"source":["new = pd.DataFrame()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fui-VtEdxyA2","executionInfo":{"elapsed":2424,"status":"ok","timestamp":1633244136414,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"c7dead31-24dd-4cf2-ac80-c09270570237"},"source":["for file in category:\n","    df = pd.read_csv(csv_path+file+'.csv', index_col=0)\n","    print(file, len(df))\n","    new = pd.concat([new, df])\n","    new.reset_index(inplace=True, drop=True)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["개인및관계 71408\n","미용과건강 16520\n","상거래(쇼핑) 25480\n","시사교육 14840\n","식음료 29400\n","여가생활 35840\n","일과직업 20152\n","주거와생활 45640\n","행사 20720\n"]}]},{"cell_type":"markdown","metadata":{"id":"Se68UiQAh9UN"},"source":["> 특수문자 제거하기\n","\n","참고: https://sikaleo.tistory.com/64"]},{"cell_type":"code","metadata":{"id":"Xb0zpVqVbJeE"},"source":["import re\n","def cleasing(text):\n","  repl = ''\n","  #pattern = '[^\\w\\s]' # 특수기호 제거\n","  pattern = '[^가-히\\s]' # 특수기호 제거\n","  text = re.sub(pattern= pattern, repl=repl, string=text)\n","  return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LX21rKFchaG2"},"source":["new['text'] = new['text'].map(lambda x: cleasing(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4uVbHeqiiMCn"},"source":["from sklearn.model_selection import train_test_split\n","train, test = train_test_split(new, test_size=0.1, random_state=20)\n","train.to_csv(path_or_buf=csv_path+'korean_dialogue_train.csv', encoding = 'utf-8-sig', index=False)\n","test.to_csv(path_or_buf=csv_path+'korean_dialogue_test.csv', encoding = 'utf-8-sig', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wK51BJp5p3Me"},"source":["### 일상대화 모델링"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MqLGWpfMp3Mf","executionInfo":{"elapsed":33,"status":"ok","timestamp":1633819763495,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"6a64591b-0783-4675-82ee-47a5ce4bb6c0"},"source":["cd '/content/drive/MyDrive/munning_rachine'"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/munning_rachine\n"]}]},{"cell_type":"code","metadata":{"id":"6Jwwi9Ysp3Mg"},"source":["import time\n","import datetime\n","import torch\n","import gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"1PQmYdYdp3Mg","outputId":"091f434a-7b77-4c23-fb2f-d29f64a80480"},"source":["start = time.time()\n","#!python KoBART/train.py \\\n","--train_file='dataset/korean_dialogue/Training/csv/korean_dialogue_train.csv' \\\n","--test_file='dataset/korean_dialogue/Training/csv/korean_dialogue_test.csv' \\\n","--mode='train' \\\n","--batch_size=10 \\\n","--num_workers=2 \\\n","--gradient_clip_val=1.0 \\\n","--gpus=1 \\\n","--accelerator=ddp \\\n","--max_epochs=5 \\\n","--default_root_dir='KoBART/model/korean_dialogue/train'\n","\n","sec = time.time()-start\n","times = str(datetime.timedelta(seconds=sec)).split(\".\")\n","times = times[0]\n","print(times)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:root:Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=10, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path=None, default_root_dir='KoBART/model/korean_dialogue/train', deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=1.0, hparams_file=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=5, max_len=512, max_steps=None, min_epochs=None, min_steps=None, mode='train', model_path=None, move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test_file='dataset/korean_dialogue/Training/csv/korean_dialogue_test.csv', tpu_cores=<function _gpus_arg_default at 0x7f0de9e90050>, track_grad_norm=-1, train_file='dataset/korean_dialogue/Training/csv/korean_dialogue_train.csv', truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n","[██████████████████████████████████████████████████]\n","GPU available: True, used: True\n","INFO:lightning:GPU available: True, used: True\n","TPU available: None, using: 0 TPU cores\n","INFO:lightning:TPU available: None, using: 0 TPU cores\n","[██████████████████████████████████████████████████]\n","using cached model\n","INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:root:number of workers 2, data length 252000\n","INFO:root:num_train_steps : 63000\n","INFO:root:num_warmup_steps : 6300\n","initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n","INFO:lightning:initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n","\n","  | Name  | Type                         | Params\n","-------------------------------------------------------\n","0 | model | BartForConditionalGeneration | 123 M \n","-------------------------------------------------------\n","123 M     Trainable params\n","0         Non-trainable params\n","123 M     Total params\n","495.440   Total estimated model params size (MB)\n","INFO:lightning:\n","  | Name  | Type                         | Params\n","-------------------------------------------------------\n","0 | model | BartForConditionalGeneration | 123 M \n","-------------------------------------------------------\n","123 M     Trainable params\n","0         Non-trainable params\n","123 M     Total params\n","495.440   Total estimated model params size (MB)\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0% 0/28000 [00:00<?, ?it/s] INFO:root:Reducer buckets have been rebuilt in this iteration.\n","Epoch 0:  90% 25220/28000 [3:20:52<22:08,  2.09it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/2800 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  90% 25240/28000 [3:20:55<21:58,  2.09it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  90% 25260/28000 [3:20:58<21:48,  2.09it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  90% 25280/28000 [3:21:01<21:37,  2.10it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  90% 25300/28000 [3:21:05<21:27,  2.10it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  90% 25320/28000 [3:21:08<21:17,  2.10it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  90% 25340/28000 [3:21:11<21:07,  2.10it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  91% 25360/28000 [3:21:14<20:56,  2.10it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  91% 25380/28000 [3:21:17<20:46,  2.10it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  91% 25400/28000 [3:21:20<20:36,  2.10it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  91% 25420/28000 [3:21:23<20:26,  2.10it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  91% 25440/28000 [3:21:26<20:16,  2.10it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  91% 25460/28000 [3:21:29<20:06,  2.11it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  91% 25480/28000 [3:21:33<19:56,  2.11it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  91% 25500/28000 [3:21:36<19:45,  2.11it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  91% 25520/28000 [3:21:39<19:35,  2.11it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  91% 25540/28000 [3:21:42<19:25,  2.11it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  91% 25560/28000 [3:21:45<19:15,  2.11it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  91% 25580/28000 [3:21:48<19:05,  2.11it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  91% 25600/28000 [3:21:51<18:55,  2.11it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  92% 25620/28000 [3:21:54<18:45,  2.11it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  92% 25640/28000 [3:21:58<18:35,  2.12it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  92% 25660/28000 [3:22:01<18:25,  2.12it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  92% 25680/28000 [3:22:04<18:15,  2.12it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  92% 25700/28000 [3:22:07<18:05,  2.12it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  92% 25720/28000 [3:22:10<17:55,  2.12it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  92% 25740/28000 [3:22:13<17:45,  2.12it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  92% 25760/28000 [3:22:16<17:35,  2.12it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  92% 25780/28000 [3:22:19<17:25,  2.12it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  92% 25800/28000 [3:22:23<17:15,  2.12it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  92% 25820/28000 [3:22:26<17:05,  2.13it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  92% 25840/28000 [3:22:29<16:55,  2.13it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  92% 25860/28000 [3:22:32<16:45,  2.13it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  92% 25880/28000 [3:22:35<16:35,  2.13it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  92% 25900/28000 [3:22:38<16:25,  2.13it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  93% 25920/28000 [3:22:41<16:15,  2.13it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  93% 25940/28000 [3:22:44<16:06,  2.13it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  93% 25960/28000 [3:22:47<15:56,  2.13it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  93% 25980/28000 [3:22:51<15:46,  2.13it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  93% 26000/28000 [3:22:54<15:36,  2.14it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  93% 26020/28000 [3:22:57<15:26,  2.14it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  93% 26040/28000 [3:23:00<15:16,  2.14it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  93% 26060/28000 [3:23:03<15:06,  2.14it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  93% 26080/28000 [3:23:06<14:57,  2.14it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  93% 26100/28000 [3:23:09<14:47,  2.14it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  93% 26120/28000 [3:23:12<14:37,  2.14it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  93% 26140/28000 [3:23:16<14:27,  2.14it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  93% 26160/28000 [3:23:19<14:18,  2.14it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  94% 26180/28000 [3:23:22<14:08,  2.15it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  94% 26200/28000 [3:23:25<13:58,  2.15it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  94% 26220/28000 [3:23:28<13:48,  2.15it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  94% 26240/28000 [3:23:31<13:39,  2.15it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  94% 26260/28000 [3:23:34<13:29,  2.15it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  94% 26280/28000 [3:23:37<13:19,  2.15it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  94% 26300/28000 [3:23:41<13:09,  2.15it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  94% 26320/28000 [3:23:44<13:00,  2.15it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  94% 26340/28000 [3:23:47<12:50,  2.15it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  94% 26360/28000 [3:23:50<12:40,  2.16it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  94% 26380/28000 [3:23:53<12:31,  2.16it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  94% 26400/28000 [3:23:56<12:21,  2.16it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  94% 26420/28000 [3:23:59<12:11,  2.16it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  94% 26440/28000 [3:24:02<12:02,  2.16it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  94% 26460/28000 [3:24:05<11:52,  2.16it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  95% 26480/28000 [3:24:09<11:43,  2.16it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  95% 26500/28000 [3:24:12<11:33,  2.16it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  95% 26520/28000 [3:24:15<11:23,  2.16it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  95% 26540/28000 [3:24:18<11:14,  2.17it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  95% 26560/28000 [3:24:21<11:04,  2.17it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  95% 26580/28000 [3:24:24<10:55,  2.17it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  95% 26600/28000 [3:24:27<10:45,  2.17it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  95% 26620/28000 [3:24:30<10:36,  2.17it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  95% 26640/28000 [3:24:34<10:26,  2.17it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  95% 26660/28000 [3:24:37<10:17,  2.17it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  95% 26680/28000 [3:24:40<10:07,  2.17it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  95% 26700/28000 [3:24:43<09:58,  2.17it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  95% 26720/28000 [3:24:46<09:48,  2.17it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  96% 26740/28000 [3:24:49<09:39,  2.18it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  96% 26760/28000 [3:24:52<09:29,  2.18it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  96% 26780/28000 [3:24:55<09:20,  2.18it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  96% 26800/28000 [3:24:59<09:10,  2.18it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  96% 26820/28000 [3:25:02<09:01,  2.18it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  96% 26840/28000 [3:25:05<08:51,  2.18it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  96% 26860/28000 [3:25:08<08:42,  2.18it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  96% 26880/28000 [3:25:11<08:32,  2.18it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  96% 26900/28000 [3:25:14<08:23,  2.18it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  96% 26920/28000 [3:25:17<08:14,  2.19it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  96% 26940/28000 [3:25:20<08:04,  2.19it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  96% 26960/28000 [3:25:23<07:55,  2.19it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  96% 26980/28000 [3:25:27<07:46,  2.19it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  96% 27000/28000 [3:25:30<07:36,  2.19it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  96% 27020/28000 [3:25:33<07:27,  2.19it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  97% 27040/28000 [3:25:36<07:17,  2.19it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  97% 27060/28000 [3:25:39<07:08,  2.19it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  97% 27080/28000 [3:25:42<06:59,  2.19it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  97% 27100/28000 [3:25:45<06:50,  2.20it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  97% 27120/28000 [3:25:48<06:40,  2.20it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  97% 27140/28000 [3:25:52<06:31,  2.20it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  97% 27160/28000 [3:25:55<06:22,  2.20it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  97% 27180/28000 [3:25:58<06:12,  2.20it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  97% 27200/28000 [3:26:01<06:03,  2.20it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  97% 27220/28000 [3:26:04<05:54,  2.20it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  97% 27240/28000 [3:26:07<05:45,  2.20it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  97% 27260/28000 [3:26:10<05:35,  2.20it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  97% 27280/28000 [3:26:13<05:26,  2.20it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  98% 27300/28000 [3:26:16<05:17,  2.21it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  98% 27320/28000 [3:26:20<05:08,  2.21it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  98% 27340/28000 [3:26:23<04:58,  2.21it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  98% 27360/28000 [3:26:26<04:49,  2.21it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  98% 27380/28000 [3:26:29<04:40,  2.21it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  98% 27400/28000 [3:26:32<04:31,  2.21it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  98% 27420/28000 [3:26:35<04:22,  2.21it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  98% 27440/28000 [3:26:38<04:13,  2.21it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  98% 27460/28000 [3:26:41<04:03,  2.21it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  98% 27480/28000 [3:26:45<03:54,  2.22it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  98% 27500/28000 [3:26:48<03:45,  2.22it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  98% 27520/28000 [3:26:51<03:36,  2.22it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  98% 27540/28000 [3:26:54<03:27,  2.22it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  98% 27560/28000 [3:26:57<03:18,  2.22it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  98% 27580/28000 [3:27:00<03:09,  2.22it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  99% 27600/28000 [3:27:03<03:00,  2.22it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  99% 27620/28000 [3:27:06<02:50,  2.22it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  99% 27640/28000 [3:27:10<02:41,  2.22it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  99% 27660/28000 [3:27:13<02:32,  2.22it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  99% 27680/28000 [3:27:16<02:23,  2.23it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  99% 27700/28000 [3:27:19<02:14,  2.23it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  99% 27720/28000 [3:27:22<02:05,  2.23it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  99% 27740/28000 [3:27:25<01:56,  2.23it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  99% 27760/28000 [3:27:28<01:47,  2.23it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  99% 27780/28000 [3:27:31<01:38,  2.23it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  99% 27800/28000 [3:27:34<01:29,  2.23it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  99% 27820/28000 [3:27:38<01:20,  2.23it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0:  99% 27840/28000 [3:27:41<01:11,  2.23it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0: 100% 27860/28000 [3:27:44<01:02,  2.24it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0: 100% 27880/28000 [3:27:47<00:53,  2.24it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0: 100% 27900/28000 [3:27:50<00:44,  2.24it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0: 100% 27920/28000 [3:27:53<00:35,  2.24it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0: 100% 27940/28000 [3:27:56<00:26,  2.24it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0: 100% 27960/28000 [3:27:59<00:17,  2.24it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0: 100% 27980/28000 [3:28:03<00:08,  2.24it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0: 100% 28000/28000 [3:28:06<00:00,  2.24it/s, loss=2.17, v_num=3, val_loss=10.70, train_loss=1.990]\n","Epoch 0: 100% 28000/28000 [3:28:09<00:00,  2.24it/s, loss=2.17, v_num=3, val_loss=2.020, train_loss=2.020]\n","                                                   \u001b[AEpoch 0, global step 25199: val_loss reached 2.01511 (best 2.01511), saving model to \"/content/drive/My Drive/munning_rachine/KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=00-val_loss=2.015.ckpt\" as top 1\n","INFO:lightning:Epoch 0, global step 25199: val_loss reached 2.01511 (best 2.01511), saving model to \"/content/drive/My Drive/munning_rachine/KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=00-val_loss=2.015.ckpt\" as top 1\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","tcmalloc: large alloc 1320878080 bytes == 0x55d6306a4000 @  0x7f0e87359615 0x55d51942c4cc 0x55d51950c47a 0x55d519432f0c 0x7f0e3ae1ea94 0x7f0e3ae20864 0x7f0e3adf0590 0x7f0e2bb52465 0x7f0e2bb4e9ca 0x7f0e2bb53609 0x7f0e3adf3f2b 0x7f0e3aa79200 0x55d519430098 0x55d5194a34d9 0x55d51949dced 0x55d519430bda 0x55d51949e915 0x55d51949d9ee 0x55d519430bda 0x55d5194a2d00 0x55d519430afa 0x55d51949e915 0x55d51949d9ee 0x55d519430bda 0x55d51949ec0d 0x55d51949d9ee 0x55d519430bda 0x55d5194a2d00 0x55d519430afa 0x55d51949ec0d 0x55d519430afa\n","tcmalloc: large alloc 1651097600 bytes == 0x55d5c5830000 @  0x7f0e87359615 0x55d51942c4cc 0x55d51950c47a 0x55d519432f0c 0x7f0e3ae1ea94 0x7f0e3ae20864 0x7f0e3adf0590 0x7f0e2bb52465 0x7f0e2bb4e9ca 0x7f0e2bb53609 0x7f0e3adf3f2b 0x7f0e3aa79200 0x55d519430098 0x55d5194a34d9 0x55d51949dced 0x55d519430bda 0x55d51949e915 0x55d51949d9ee 0x55d519430bda 0x55d5194a2d00 0x55d519430afa 0x55d51949e915 0x55d51949d9ee 0x55d519430bda 0x55d51949ec0d 0x55d51949d9ee 0x55d519430bda 0x55d5194a2d00 0x55d519430afa 0x55d51949ec0d 0x55d519430afa\n","Epoch 0: 100% 28000/28000 [3:28:19<00:00,  2.24it/s, loss=2.17, v_num=3, val_loss=2.020, train_loss=2.020]tcmalloc: large alloc 1651097600 bytes == 0x55d6306a4000 @  0x7f0e87359615 0x55d51942c4cc 0x55d51950c47a 0x55d519432f0c 0x7f0e3ae1ea94 0x7f0e3ae20864 0x7f0e3adf0590 0x7f0e2bb52465 0x7f0e2bb4e9ca 0x7f0e2bb53609 0x7f0e3adf3f2b 0x7f0e3aa79200 0x55d519430098 0x55d5194a34d9 0x55d51949dced 0x55d519430bda 0x55d51949e915 0x55d51949d9ee 0x55d519430bda 0x55d5194a2d00 0x55d519430afa 0x55d51949e915 0x55d51949d9ee 0x55d519430bda 0x55d51949ec0d 0x55d51949d9ee 0x55d519430bda 0x55d5194a2d00 0x55d519430afa 0x55d51949ec0d 0x55d519430afa\n","Epoch 1:  90% 25220/28000 [3:18:58<21:55,  2.11it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/2800 [00:00<?, ?it/s]\u001b[A\n","Epoch 1:  90% 25240/28000 [3:19:01<21:45,  2.11it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  90% 25260/28000 [3:19:04<21:35,  2.11it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  90% 25280/28000 [3:19:07<21:25,  2.12it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  90% 25300/28000 [3:19:10<21:15,  2.12it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  90% 25320/28000 [3:19:13<21:05,  2.12it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  90% 25340/28000 [3:19:16<20:55,  2.12it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  91% 25360/28000 [3:19:20<20:45,  2.12it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  91% 25380/28000 [3:19:23<20:34,  2.12it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  91% 25400/28000 [3:19:26<20:24,  2.12it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  91% 25420/28000 [3:19:29<20:14,  2.12it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  91% 25440/28000 [3:19:32<20:04,  2.12it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  91% 25460/28000 [3:19:35<19:54,  2.13it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  91% 25480/28000 [3:19:38<19:44,  2.13it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  91% 25500/28000 [3:19:41<19:34,  2.13it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  91% 25520/28000 [3:19:44<19:24,  2.13it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  91% 25540/28000 [3:19:48<19:14,  2.13it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  91% 25560/28000 [3:19:51<19:04,  2.13it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  91% 25580/28000 [3:19:54<18:54,  2.13it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  91% 25600/28000 [3:19:57<18:44,  2.13it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  92% 25620/28000 [3:20:00<18:34,  2.13it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  92% 25640/28000 [3:20:03<18:24,  2.14it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  92% 25660/28000 [3:20:06<18:14,  2.14it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  92% 25680/28000 [3:20:09<18:05,  2.14it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  92% 25700/28000 [3:20:13<17:55,  2.14it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  92% 25720/28000 [3:20:16<17:45,  2.14it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  92% 25740/28000 [3:20:19<17:35,  2.14it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  92% 25760/28000 [3:20:22<17:25,  2.14it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  92% 25780/28000 [3:20:25<17:15,  2.14it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  92% 25800/28000 [3:20:28<17:05,  2.14it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  92% 25820/28000 [3:20:31<16:55,  2.15it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  92% 25840/28000 [3:20:34<16:46,  2.15it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  92% 25860/28000 [3:20:37<16:36,  2.15it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  92% 25880/28000 [3:20:41<16:26,  2.15it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  92% 25900/28000 [3:20:44<16:16,  2.15it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  93% 25920/28000 [3:20:47<16:06,  2.15it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  93% 25940/28000 [3:20:50<15:56,  2.15it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  93% 25960/28000 [3:20:53<15:47,  2.15it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  93% 25980/28000 [3:20:56<15:37,  2.15it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  93% 26000/28000 [3:20:59<15:27,  2.16it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  93% 26020/28000 [3:21:02<15:17,  2.16it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  93% 26040/28000 [3:21:06<15:08,  2.16it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  93% 26060/28000 [3:21:09<14:58,  2.16it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  93% 26080/28000 [3:21:12<14:48,  2.16it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  93% 26100/28000 [3:21:15<14:39,  2.16it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  93% 26120/28000 [3:21:18<14:29,  2.16it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  93% 26140/28000 [3:21:21<14:19,  2.16it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  93% 26160/28000 [3:21:24<14:09,  2.16it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  94% 26180/28000 [3:21:27<14:00,  2.17it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  94% 26200/28000 [3:21:30<13:50,  2.17it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  94% 26220/28000 [3:21:34<13:41,  2.17it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  94% 26240/28000 [3:21:37<13:31,  2.17it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  94% 26260/28000 [3:21:40<13:21,  2.17it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  94% 26280/28000 [3:21:43<13:12,  2.17it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  94% 26300/28000 [3:21:46<13:02,  2.17it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  94% 26320/28000 [3:21:49<12:52,  2.17it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  94% 26340/28000 [3:21:52<12:43,  2.17it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  94% 26360/28000 [3:21:55<12:33,  2.18it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  94% 26380/28000 [3:21:59<12:24,  2.18it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  94% 26400/28000 [3:22:02<12:14,  2.18it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  94% 26420/28000 [3:22:05<12:05,  2.18it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  94% 26440/28000 [3:22:08<11:55,  2.18it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  94% 26460/28000 [3:22:11<11:46,  2.18it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  95% 26480/28000 [3:22:14<11:36,  2.18it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  95% 26500/28000 [3:22:17<11:27,  2.18it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  95% 26520/28000 [3:22:20<11:17,  2.18it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  95% 26540/28000 [3:22:23<11:08,  2.19it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  95% 26560/28000 [3:22:27<10:58,  2.19it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  95% 26580/28000 [3:22:30<10:49,  2.19it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  95% 26600/28000 [3:22:33<10:39,  2.19it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  95% 26620/28000 [3:22:36<10:30,  2.19it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  95% 26640/28000 [3:22:39<10:20,  2.19it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  95% 26660/28000 [3:22:42<10:11,  2.19it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  95% 26680/28000 [3:22:45<10:01,  2.19it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  95% 26700/28000 [3:22:48<09:52,  2.19it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  95% 26720/28000 [3:22:52<09:43,  2.20it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  96% 26740/28000 [3:22:55<09:33,  2.20it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  96% 26760/28000 [3:22:58<09:24,  2.20it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  96% 26780/28000 [3:23:01<09:14,  2.20it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  96% 26800/28000 [3:23:04<09:05,  2.20it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  96% 26820/28000 [3:23:07<08:56,  2.20it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  96% 26840/28000 [3:23:10<08:46,  2.20it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  96% 26860/28000 [3:23:13<08:37,  2.20it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  96% 26880/28000 [3:23:16<08:28,  2.20it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  96% 26900/28000 [3:23:20<08:18,  2.20it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  96% 26920/28000 [3:23:23<08:09,  2.21it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  96% 26940/28000 [3:23:26<08:00,  2.21it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  96% 26960/28000 [3:23:29<07:50,  2.21it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  96% 26980/28000 [3:23:32<07:41,  2.21it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  96% 27000/28000 [3:23:35<07:32,  2.21it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  96% 27020/28000 [3:23:38<07:23,  2.21it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  97% 27040/28000 [3:23:41<07:13,  2.21it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  97% 27060/28000 [3:23:45<07:04,  2.21it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  97% 27080/28000 [3:23:48<06:55,  2.21it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  97% 27100/28000 [3:23:51<06:46,  2.22it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  97% 27120/28000 [3:23:54<06:36,  2.22it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  97% 27140/28000 [3:23:57<06:27,  2.22it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  97% 27160/28000 [3:24:00<06:18,  2.22it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  97% 27180/28000 [3:24:03<06:09,  2.22it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  97% 27200/28000 [3:24:06<06:00,  2.22it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  97% 27220/28000 [3:24:09<05:51,  2.22it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  97% 27240/28000 [3:24:13<05:41,  2.22it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  97% 27260/28000 [3:24:16<05:32,  2.22it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  97% 27280/28000 [3:24:19<05:23,  2.23it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  98% 27300/28000 [3:24:22<05:14,  2.23it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  98% 27320/28000 [3:24:25<05:05,  2.23it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  98% 27340/28000 [3:24:28<04:56,  2.23it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  98% 27360/28000 [3:24:31<04:47,  2.23it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  98% 27380/28000 [3:24:34<04:37,  2.23it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  98% 27400/28000 [3:24:38<04:28,  2.23it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  98% 27420/28000 [3:24:41<04:19,  2.23it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  98% 27440/28000 [3:24:44<04:10,  2.23it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  98% 27460/28000 [3:24:47<04:01,  2.23it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  98% 27480/28000 [3:24:50<03:52,  2.24it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  98% 27500/28000 [3:24:53<03:43,  2.24it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  98% 27520/28000 [3:24:56<03:34,  2.24it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  98% 27540/28000 [3:24:59<03:25,  2.24it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  98% 27560/28000 [3:25:03<03:16,  2.24it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  98% 27580/28000 [3:25:06<03:07,  2.24it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  99% 27600/28000 [3:25:09<02:58,  2.24it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  99% 27620/28000 [3:25:12<02:49,  2.24it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  99% 27640/28000 [3:25:15<02:40,  2.24it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  99% 27660/28000 [3:25:18<02:31,  2.25it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  99% 27680/28000 [3:25:21<02:22,  2.25it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  99% 27700/28000 [3:25:24<02:13,  2.25it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  99% 27720/28000 [3:25:27<02:04,  2.25it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  99% 27740/28000 [3:25:31<01:55,  2.25it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  99% 27760/28000 [3:25:34<01:46,  2.25it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  99% 27780/28000 [3:25:37<01:37,  2.25it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  99% 27800/28000 [3:25:40<01:28,  2.25it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  99% 27820/28000 [3:25:43<01:19,  2.25it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1:  99% 27840/28000 [3:25:46<01:10,  2.25it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1: 100% 27860/28000 [3:25:49<01:02,  2.26it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1: 100% 27880/28000 [3:25:52<00:53,  2.26it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1: 100% 27900/28000 [3:25:56<00:44,  2.26it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1: 100% 27920/28000 [3:25:59<00:35,  2.26it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1: 100% 27940/28000 [3:26:02<00:26,  2.26it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1: 100% 27960/28000 [3:26:05<00:17,  2.26it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1: 100% 27980/28000 [3:26:08<00:08,  2.26it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1: 100% 28000/28000 [3:26:11<00:00,  2.26it/s, loss=1.86, v_num=3, val_loss=2.020, train_loss=1.790]\n","Epoch 1: 100% 28000/28000 [3:26:14<00:00,  2.26it/s, loss=1.86, v_num=3, val_loss=1.920, train_loss=1.730]\n","                                                   \u001b[AEpoch 1, global step 50399: val_loss reached 1.91568 (best 1.91568), saving model to \"/content/drive/My Drive/munning_rachine/KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=01-val_loss=1.916.ckpt\" as top 2\n","INFO:lightning:Epoch 1, global step 50399: val_loss reached 1.91568 (best 1.91568), saving model to \"/content/drive/My Drive/munning_rachine/KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=01-val_loss=1.916.ckpt\" as top 2\n","tcmalloc: large alloc 1651097600 bytes == 0x55d6306a4000 @  0x7f0e87359615 0x55d51942c4cc 0x55d51950c47a 0x55d519432f0c 0x7f0e3ae1ea94 0x7f0e3ae20864 0x7f0e3adf0590 0x7f0e2bb52465 0x7f0e2bb4e9ca 0x7f0e2bb53609 0x7f0e3adf3f2b 0x7f0e3aa79200 0x55d519430098 0x55d5194a34d9 0x55d51949dced 0x55d519430bda 0x55d51949e915 0x55d51949d9ee 0x55d519430bda 0x55d5194a2d00 0x55d519430afa 0x55d51949e915 0x55d51949d9ee 0x55d519430bda 0x55d51949ec0d 0x55d51949d9ee 0x55d519430bda 0x55d5194a2d00 0x55d519430afa 0x55d51949ec0d 0x55d519430afa\n","Epoch 2:  90% 25220/28000 [3:18:46<21:54,  2.11it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/2800 [00:00<?, ?it/s]\u001b[A\n","Epoch 2:  90% 25240/28000 [3:18:49<21:44,  2.12it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  90% 25260/28000 [3:18:52<21:34,  2.12it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  90% 25280/28000 [3:18:55<21:24,  2.12it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  90% 25300/28000 [3:18:58<21:14,  2.12it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  90% 25320/28000 [3:19:01<21:03,  2.12it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  90% 25340/28000 [3:19:04<20:53,  2.12it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  91% 25360/28000 [3:19:07<20:43,  2.12it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  91% 25380/28000 [3:19:11<20:33,  2.12it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  91% 25400/28000 [3:19:14<20:23,  2.12it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  91% 25420/28000 [3:19:17<20:13,  2.13it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  91% 25440/28000 [3:19:20<20:03,  2.13it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  91% 25460/28000 [3:19:23<19:53,  2.13it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  91% 25480/28000 [3:19:26<19:43,  2.13it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  91% 25500/28000 [3:19:29<19:33,  2.13it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  91% 25520/28000 [3:19:32<19:23,  2.13it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  91% 25540/28000 [3:19:35<19:13,  2.13it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  91% 25560/28000 [3:19:39<19:03,  2.13it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  91% 25580/28000 [3:19:42<18:53,  2.13it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  91% 25600/28000 [3:19:45<18:43,  2.14it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  92% 25620/28000 [3:19:48<18:33,  2.14it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  92% 25640/28000 [3:19:51<18:23,  2.14it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  92% 25660/28000 [3:19:54<18:13,  2.14it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  92% 25680/28000 [3:19:57<18:03,  2.14it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  92% 25700/28000 [3:20:00<17:54,  2.14it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  92% 25720/28000 [3:20:03<17:44,  2.14it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  92% 25740/28000 [3:20:07<17:34,  2.14it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  92% 25760/28000 [3:20:10<17:24,  2.14it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  92% 25780/28000 [3:20:13<17:14,  2.15it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  92% 25800/28000 [3:20:16<17:04,  2.15it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  92% 25820/28000 [3:20:19<16:54,  2.15it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  92% 25840/28000 [3:20:22<16:44,  2.15it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  92% 25860/28000 [3:20:25<16:35,  2.15it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  92% 25880/28000 [3:20:28<16:25,  2.15it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  92% 25900/28000 [3:20:32<16:15,  2.15it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  93% 25920/28000 [3:20:35<16:05,  2.15it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  93% 25940/28000 [3:20:38<15:56,  2.15it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  93% 25960/28000 [3:20:41<15:46,  2.16it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  93% 25980/28000 [3:20:44<15:36,  2.16it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  93% 26000/28000 [3:20:47<15:26,  2.16it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  93% 26020/28000 [3:20:50<15:17,  2.16it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  93% 26040/28000 [3:20:53<15:07,  2.16it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  93% 26060/28000 [3:20:56<14:57,  2.16it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  93% 26080/28000 [3:21:00<14:47,  2.16it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  93% 26100/28000 [3:21:03<14:38,  2.16it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  93% 26120/28000 [3:21:06<14:28,  2.16it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  93% 26140/28000 [3:21:09<14:18,  2.17it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  93% 26160/28000 [3:21:12<14:09,  2.17it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  94% 26180/28000 [3:21:15<13:59,  2.17it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  94% 26200/28000 [3:21:18<13:49,  2.17it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  94% 26220/28000 [3:21:21<13:40,  2.17it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  94% 26240/28000 [3:21:24<13:30,  2.17it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  94% 26260/28000 [3:21:28<13:20,  2.17it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  94% 26280/28000 [3:21:31<13:11,  2.17it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  94% 26300/28000 [3:21:34<13:01,  2.17it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  94% 26320/28000 [3:21:37<12:52,  2.18it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  94% 26340/28000 [3:21:40<12:42,  2.18it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  94% 26360/28000 [3:21:43<12:33,  2.18it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  94% 26380/28000 [3:21:46<12:23,  2.18it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  94% 26400/28000 [3:21:49<12:13,  2.18it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  94% 26420/28000 [3:21:52<12:04,  2.18it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  94% 26440/28000 [3:21:55<11:54,  2.18it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  94% 26460/28000 [3:21:59<11:45,  2.18it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  95% 26480/28000 [3:22:02<11:35,  2.18it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  95% 26500/28000 [3:22:05<11:26,  2.19it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  95% 26520/28000 [3:22:08<11:16,  2.19it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  95% 26540/28000 [3:22:11<11:07,  2.19it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  95% 26560/28000 [3:22:14<10:57,  2.19it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  95% 26580/28000 [3:22:17<10:48,  2.19it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  95% 26600/28000 [3:22:20<10:38,  2.19it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  95% 26620/28000 [3:22:23<10:29,  2.19it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  95% 26640/28000 [3:22:26<10:20,  2.19it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  95% 26660/28000 [3:22:30<10:10,  2.19it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  95% 26680/28000 [3:22:33<10:01,  2.20it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  95% 26700/28000 [3:22:36<09:51,  2.20it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  95% 26720/28000 [3:22:39<09:42,  2.20it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  96% 26740/28000 [3:22:42<09:33,  2.20it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  96% 26760/28000 [3:22:45<09:23,  2.20it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  96% 26780/28000 [3:22:48<09:14,  2.20it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  96% 26800/28000 [3:22:51<09:05,  2.20it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  96% 26820/28000 [3:22:54<08:55,  2.20it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  96% 26840/28000 [3:22:58<08:46,  2.20it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  96% 26860/28000 [3:23:01<08:36,  2.21it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  96% 26880/28000 [3:23:04<08:27,  2.21it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  96% 26900/28000 [3:23:07<08:18,  2.21it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  96% 26920/28000 [3:23:10<08:09,  2.21it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  96% 26940/28000 [3:23:13<07:59,  2.21it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  96% 26960/28000 [3:23:16<07:50,  2.21it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  96% 26980/28000 [3:23:19<07:41,  2.21it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  96% 27000/28000 [3:23:22<07:31,  2.21it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  96% 27020/28000 [3:23:26<07:22,  2.21it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  97% 27040/28000 [3:23:29<07:13,  2.21it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  97% 27060/28000 [3:23:32<07:04,  2.22it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  97% 27080/28000 [3:23:35<06:55,  2.22it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  97% 27100/28000 [3:23:38<06:45,  2.22it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  97% 27120/28000 [3:23:41<06:36,  2.22it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  97% 27140/28000 [3:23:44<06:27,  2.22it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  97% 27160/28000 [3:23:47<06:18,  2.22it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  97% 27180/28000 [3:23:51<06:09,  2.22it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  97% 27200/28000 [3:23:54<05:59,  2.22it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  97% 27220/28000 [3:23:57<05:50,  2.22it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  97% 27240/28000 [3:24:00<05:41,  2.23it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  97% 27260/28000 [3:24:03<05:32,  2.23it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  97% 27280/28000 [3:24:06<05:23,  2.23it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  98% 27300/28000 [3:24:09<05:14,  2.23it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  98% 27320/28000 [3:24:12<05:04,  2.23it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  98% 27340/28000 [3:24:15<04:55,  2.23it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  98% 27360/28000 [3:24:19<04:46,  2.23it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  98% 27380/28000 [3:24:22<04:37,  2.23it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  98% 27400/28000 [3:24:25<04:28,  2.23it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  98% 27420/28000 [3:24:28<04:19,  2.24it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  98% 27440/28000 [3:24:31<04:10,  2.24it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  98% 27460/28000 [3:24:34<04:01,  2.24it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  98% 27480/28000 [3:24:37<03:52,  2.24it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  98% 27500/28000 [3:24:40<03:43,  2.24it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  98% 27520/28000 [3:24:44<03:34,  2.24it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  98% 27540/28000 [3:24:47<03:25,  2.24it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  98% 27560/28000 [3:24:50<03:16,  2.24it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  98% 27580/28000 [3:24:53<03:07,  2.24it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  99% 27600/28000 [3:24:56<02:58,  2.24it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  99% 27620/28000 [3:24:59<02:49,  2.25it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  99% 27640/28000 [3:25:02<02:40,  2.25it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  99% 27660/28000 [3:25:05<02:31,  2.25it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  99% 27680/28000 [3:25:08<02:22,  2.25it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  99% 27700/28000 [3:25:12<02:13,  2.25it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  99% 27720/28000 [3:25:15<02:04,  2.25it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  99% 27740/28000 [3:25:18<01:55,  2.25it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  99% 27760/28000 [3:25:21<01:46,  2.25it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  99% 27780/28000 [3:25:24<01:37,  2.25it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  99% 27800/28000 [3:25:27<01:28,  2.26it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  99% 27820/28000 [3:25:30<01:19,  2.26it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2:  99% 27840/28000 [3:25:33<01:10,  2.26it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2: 100% 27860/28000 [3:25:36<01:01,  2.26it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2: 100% 27880/28000 [3:25:40<00:53,  2.26it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2: 100% 27900/28000 [3:25:43<00:44,  2.26it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2: 100% 27920/28000 [3:25:46<00:35,  2.26it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2: 100% 27940/28000 [3:25:49<00:26,  2.26it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2: 100% 27960/28000 [3:25:52<00:17,  2.26it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2: 100% 27980/28000 [3:25:55<00:08,  2.26it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2: 100% 28000/28000 [3:25:58<00:00,  2.27it/s, loss=1.62, v_num=3, val_loss=1.920, train_loss=1.380]\n","Epoch 2: 100% 28000/28000 [3:26:02<00:00,  2.27it/s, loss=1.62, v_num=3, val_loss=1.930, train_loss=1.370]\n","                                                   \u001b[AEpoch 2, global step 75599: val_loss reached 1.92830 (best 1.91568), saving model to \"/content/drive/My Drive/munning_rachine/KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=02-val_loss=1.928.ckpt\" as top 3\n","INFO:lightning:Epoch 2, global step 75599: val_loss reached 1.92830 (best 1.91568), saving model to \"/content/drive/My Drive/munning_rachine/KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=02-val_loss=1.928.ckpt\" as top 3\n","Epoch 3:  90% 25220/28000 [3:18:54<21:55,  2.11it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/2800 [00:00<?, ?it/s]\u001b[A\n","Epoch 3:  90% 25240/28000 [3:18:57<21:45,  2.11it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  90% 25260/28000 [3:19:00<21:35,  2.12it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  90% 25280/28000 [3:19:03<21:25,  2.12it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  90% 25300/28000 [3:19:07<21:14,  2.12it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  90% 25320/28000 [3:19:10<21:04,  2.12it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  90% 25340/28000 [3:19:13<20:54,  2.12it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  91% 25360/28000 [3:19:16<20:44,  2.12it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  91% 25380/28000 [3:19:19<20:34,  2.12it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  91% 25400/28000 [3:19:22<20:24,  2.12it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  91% 25420/28000 [3:19:25<20:14,  2.12it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  91% 25440/28000 [3:19:28<20:04,  2.13it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  91% 25460/28000 [3:19:31<19:54,  2.13it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  91% 25480/28000 [3:19:35<19:44,  2.13it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  91% 25500/28000 [3:19:38<19:34,  2.13it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  91% 25520/28000 [3:19:41<19:24,  2.13it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  91% 25540/28000 [3:19:44<19:14,  2.13it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  91% 25560/28000 [3:19:47<19:04,  2.13it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  91% 25580/28000 [3:19:50<18:54,  2.13it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  91% 25600/28000 [3:19:53<18:44,  2.13it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  92% 25620/28000 [3:19:56<18:34,  2.14it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  92% 25640/28000 [3:19:59<18:24,  2.14it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  92% 25660/28000 [3:20:02<18:14,  2.14it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  92% 25680/28000 [3:20:06<18:04,  2.14it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  92% 25700/28000 [3:20:09<17:54,  2.14it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  92% 25720/28000 [3:20:12<17:44,  2.14it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  92% 25740/28000 [3:20:15<17:34,  2.14it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  92% 25760/28000 [3:20:18<17:25,  2.14it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  92% 25780/28000 [3:20:21<17:15,  2.14it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  92% 25800/28000 [3:20:24<17:05,  2.15it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  92% 25820/28000 [3:20:27<16:55,  2.15it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  92% 25840/28000 [3:20:31<16:45,  2.15it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  92% 25860/28000 [3:20:34<16:35,  2.15it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  92% 25880/28000 [3:20:37<16:26,  2.15it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  92% 25900/28000 [3:20:40<16:16,  2.15it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  93% 25920/28000 [3:20:43<16:06,  2.15it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  93% 25940/28000 [3:20:46<15:56,  2.15it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  93% 25960/28000 [3:20:49<15:46,  2.15it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  93% 25980/28000 [3:20:52<15:37,  2.16it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  93% 26000/28000 [3:20:55<15:27,  2.16it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  93% 26020/28000 [3:20:59<15:17,  2.16it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  93% 26040/28000 [3:21:02<15:07,  2.16it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  93% 26060/28000 [3:21:05<14:58,  2.16it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  93% 26080/28000 [3:21:08<14:48,  2.16it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  93% 26100/28000 [3:21:11<14:38,  2.16it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  93% 26120/28000 [3:21:14<14:29,  2.16it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  93% 26140/28000 [3:21:17<14:19,  2.16it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  93% 26160/28000 [3:21:20<14:09,  2.17it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  94% 26180/28000 [3:21:24<14:00,  2.17it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  94% 26200/28000 [3:21:27<13:50,  2.17it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  94% 26220/28000 [3:21:30<13:40,  2.17it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  94% 26240/28000 [3:21:33<13:31,  2.17it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  94% 26260/28000 [3:21:36<13:21,  2.17it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  94% 26280/28000 [3:21:39<13:11,  2.17it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  94% 26300/28000 [3:21:42<13:02,  2.17it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  94% 26320/28000 [3:21:45<12:52,  2.17it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  94% 26340/28000 [3:21:48<12:43,  2.18it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  94% 26360/28000 [3:21:52<12:33,  2.18it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  94% 26380/28000 [3:21:55<12:23,  2.18it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  94% 26400/28000 [3:21:58<12:14,  2.18it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  94% 26420/28000 [3:22:01<12:04,  2.18it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  94% 26440/28000 [3:22:04<11:55,  2.18it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  94% 26460/28000 [3:22:07<11:45,  2.18it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  95% 26480/28000 [3:22:10<11:36,  2.18it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  95% 26500/28000 [3:22:13<11:26,  2.18it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  95% 26520/28000 [3:22:17<11:17,  2.19it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  95% 26540/28000 [3:22:20<11:07,  2.19it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  95% 26560/28000 [3:22:23<10:58,  2.19it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  95% 26580/28000 [3:22:26<10:48,  2.19it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  95% 26600/28000 [3:22:29<10:39,  2.19it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  95% 26620/28000 [3:22:32<10:30,  2.19it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  95% 26640/28000 [3:22:35<10:20,  2.19it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  95% 26660/28000 [3:22:38<10:11,  2.19it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  95% 26680/28000 [3:22:41<10:01,  2.19it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  95% 26700/28000 [3:22:45<09:52,  2.19it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  95% 26720/28000 [3:22:48<09:42,  2.20it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  96% 26740/28000 [3:22:51<09:33,  2.20it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  96% 26760/28000 [3:22:54<09:24,  2.20it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  96% 26780/28000 [3:22:57<09:14,  2.20it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  96% 26800/28000 [3:23:00<09:05,  2.20it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  96% 26820/28000 [3:23:03<08:56,  2.20it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  96% 26840/28000 [3:23:06<08:46,  2.20it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  96% 26860/28000 [3:23:10<08:37,  2.20it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  96% 26880/28000 [3:23:13<08:28,  2.20it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  96% 26900/28000 [3:23:16<08:18,  2.21it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  96% 26920/28000 [3:23:19<08:09,  2.21it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  96% 26940/28000 [3:23:22<08:00,  2.21it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  96% 26960/28000 [3:23:25<07:50,  2.21it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  96% 26980/28000 [3:23:28<07:41,  2.21it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  96% 27000/28000 [3:23:31<07:32,  2.21it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  96% 27020/28000 [3:23:34<07:23,  2.21it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  97% 27040/28000 [3:23:38<07:13,  2.21it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  97% 27060/28000 [3:23:41<07:04,  2.21it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  97% 27080/28000 [3:23:44<06:55,  2.22it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  97% 27100/28000 [3:23:47<06:46,  2.22it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  97% 27120/28000 [3:23:50<06:36,  2.22it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  97% 27140/28000 [3:23:53<06:27,  2.22it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  97% 27160/28000 [3:23:56<06:18,  2.22it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  97% 27180/28000 [3:23:59<06:09,  2.22it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  97% 27200/28000 [3:24:02<06:00,  2.22it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  97% 27220/28000 [3:24:06<05:50,  2.22it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  97% 27240/28000 [3:24:09<05:41,  2.22it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  97% 27260/28000 [3:24:12<05:32,  2.22it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  97% 27280/28000 [3:24:15<05:23,  2.23it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  98% 27300/28000 [3:24:18<05:14,  2.23it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  98% 27320/28000 [3:24:21<05:05,  2.23it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  98% 27340/28000 [3:24:24<04:56,  2.23it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  98% 27360/28000 [3:24:27<04:46,  2.23it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  98% 27380/28000 [3:24:31<04:37,  2.23it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  98% 27400/28000 [3:24:34<04:28,  2.23it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  98% 27420/28000 [3:24:37<04:19,  2.23it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  98% 27440/28000 [3:24:40<04:10,  2.23it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  98% 27460/28000 [3:24:43<04:01,  2.24it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  98% 27480/28000 [3:24:46<03:52,  2.24it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  98% 27500/28000 [3:24:49<03:43,  2.24it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  98% 27520/28000 [3:24:52<03:34,  2.24it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  98% 27540/28000 [3:24:55<03:25,  2.24it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  98% 27560/28000 [3:24:59<03:16,  2.24it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  98% 27580/28000 [3:25:02<03:07,  2.24it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  99% 27600/28000 [3:25:05<02:58,  2.24it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  99% 27620/28000 [3:25:08<02:49,  2.24it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  99% 27640/28000 [3:25:11<02:40,  2.25it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  99% 27660/28000 [3:25:14<02:31,  2.25it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  99% 27680/28000 [3:25:17<02:22,  2.25it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  99% 27700/28000 [3:25:20<02:13,  2.25it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  99% 27720/28000 [3:25:23<02:04,  2.25it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  99% 27740/28000 [3:25:27<01:55,  2.25it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  99% 27760/28000 [3:25:30<01:46,  2.25it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  99% 27780/28000 [3:25:33<01:37,  2.25it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  99% 27800/28000 [3:25:36<01:28,  2.25it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  99% 27820/28000 [3:25:39<01:19,  2.25it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3:  99% 27840/28000 [3:25:42<01:10,  2.26it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3: 100% 27860/28000 [3:25:45<01:02,  2.26it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3: 100% 27880/28000 [3:25:48<00:53,  2.26it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3: 100% 27900/28000 [3:25:52<00:44,  2.26it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3: 100% 27920/28000 [3:25:55<00:35,  2.26it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3: 100% 27940/28000 [3:25:58<00:26,  2.26it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3: 100% 27960/28000 [3:26:01<00:17,  2.26it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3: 100% 27980/28000 [3:26:04<00:08,  2.26it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3: 100% 28000/28000 [3:26:07<00:00,  2.26it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=2.000]\n","Epoch 3: 100% 28000/28000 [3:26:10<00:00,  2.26it/s, loss=1.81, v_num=3, val_loss=1.930, train_loss=1.980]\n","                                                   \u001b[AEpoch 3, global step 100799: val_loss reached 1.93100 (best 1.91568), saving model to \"/content/drive/My Drive/munning_rachine/KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=03-val_loss=1.931.ckpt\" as top 4\n","INFO:lightning:Epoch 3, global step 100799: val_loss reached 1.93100 (best 1.91568), saving model to \"/content/drive/My Drive/munning_rachine/KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=03-val_loss=1.931.ckpt\" as top 4\n","Epoch 4:  90% 25220/28000 [3:18:53<21:55,  2.11it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/2800 [00:00<?, ?it/s]\u001b[A\n","Epoch 4:  90% 25240/28000 [3:18:56<21:45,  2.11it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  90% 25260/28000 [3:19:00<21:35,  2.12it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  90% 25280/28000 [3:19:03<21:25,  2.12it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  90% 25300/28000 [3:19:06<21:14,  2.12it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  90% 25320/28000 [3:19:09<21:04,  2.12it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  90% 25340/28000 [3:19:12<20:54,  2.12it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  91% 25360/28000 [3:19:15<20:44,  2.12it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  91% 25380/28000 [3:19:18<20:34,  2.12it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  91% 25400/28000 [3:19:22<20:24,  2.12it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  91% 25420/28000 [3:19:25<20:14,  2.12it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  91% 25440/28000 [3:19:28<20:04,  2.13it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  91% 25460/28000 [3:19:31<19:54,  2.13it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  91% 25480/28000 [3:19:34<19:44,  2.13it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  91% 25500/28000 [3:19:37<19:34,  2.13it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  91% 25520/28000 [3:19:41<19:24,  2.13it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  91% 25540/28000 [3:19:44<19:14,  2.13it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  91% 25560/28000 [3:19:47<19:04,  2.13it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  91% 25580/28000 [3:19:50<18:54,  2.13it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  91% 25600/28000 [3:19:53<18:44,  2.13it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  92% 25620/28000 [3:19:56<18:34,  2.14it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  92% 25640/28000 [3:20:00<18:24,  2.14it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  92% 25660/28000 [3:20:03<18:14,  2.14it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  92% 25680/28000 [3:20:06<18:04,  2.14it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  92% 25700/28000 [3:20:09<17:54,  2.14it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  92% 25720/28000 [3:20:12<17:44,  2.14it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  92% 25740/28000 [3:20:15<17:35,  2.14it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  92% 25760/28000 [3:20:19<17:25,  2.14it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  92% 25780/28000 [3:20:22<17:15,  2.14it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  92% 25800/28000 [3:20:25<17:05,  2.15it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  92% 25820/28000 [3:20:28<16:55,  2.15it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  92% 25840/28000 [3:20:31<16:45,  2.15it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  92% 25860/28000 [3:20:35<16:35,  2.15it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  92% 25880/28000 [3:20:38<16:26,  2.15it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  92% 25900/28000 [3:20:41<16:16,  2.15it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  93% 25920/28000 [3:20:44<16:06,  2.15it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  93% 25940/28000 [3:20:47<15:56,  2.15it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  93% 25960/28000 [3:20:50<15:46,  2.15it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  93% 25980/28000 [3:20:54<15:37,  2.16it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  93% 26000/28000 [3:20:57<15:27,  2.16it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  93% 26020/28000 [3:21:00<15:17,  2.16it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  93% 26040/28000 [3:21:03<15:08,  2.16it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  93% 26060/28000 [3:21:06<14:58,  2.16it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  93% 26080/28000 [3:21:09<14:48,  2.16it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  93% 26100/28000 [3:21:13<14:38,  2.16it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  93% 26120/28000 [3:21:16<14:29,  2.16it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  93% 26140/28000 [3:21:19<14:19,  2.16it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  93% 26160/28000 [3:21:22<14:09,  2.17it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  94% 26180/28000 [3:21:25<14:00,  2.17it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  94% 26200/28000 [3:21:28<13:50,  2.17it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  94% 26220/28000 [3:21:32<13:40,  2.17it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  94% 26240/28000 [3:21:35<13:31,  2.17it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  94% 26260/28000 [3:21:38<13:21,  2.17it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  94% 26280/28000 [3:21:41<13:12,  2.17it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  94% 26300/28000 [3:21:44<13:02,  2.17it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  94% 26320/28000 [3:21:47<12:52,  2.17it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  94% 26340/28000 [3:21:51<12:43,  2.17it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  94% 26360/28000 [3:21:54<12:33,  2.18it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  94% 26380/28000 [3:21:57<12:24,  2.18it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  94% 26400/28000 [3:22:00<12:14,  2.18it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  94% 26420/28000 [3:22:03<12:05,  2.18it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  94% 26440/28000 [3:22:07<11:55,  2.18it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  94% 26460/28000 [3:22:10<11:45,  2.18it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  95% 26480/28000 [3:22:13<11:36,  2.18it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  95% 26500/28000 [3:22:16<11:26,  2.18it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  95% 26520/28000 [3:22:19<11:17,  2.18it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  95% 26540/28000 [3:22:22<11:07,  2.19it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  95% 26560/28000 [3:22:26<10:58,  2.19it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  95% 26580/28000 [3:22:29<10:49,  2.19it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  95% 26600/28000 [3:22:32<10:39,  2.19it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  95% 26620/28000 [3:22:35<10:30,  2.19it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  95% 26640/28000 [3:22:38<10:20,  2.19it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  95% 26660/28000 [3:22:41<10:11,  2.19it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  95% 26680/28000 [3:22:45<10:01,  2.19it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  95% 26700/28000 [3:22:48<09:52,  2.19it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  95% 26720/28000 [3:22:51<09:43,  2.20it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  96% 26740/28000 [3:22:54<09:33,  2.20it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  96% 26760/28000 [3:22:57<09:24,  2.20it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  96% 26780/28000 [3:23:00<09:14,  2.20it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  96% 26800/28000 [3:23:04<09:05,  2.20it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  96% 26820/28000 [3:23:07<08:56,  2.20it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  96% 26840/28000 [3:23:10<08:46,  2.20it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  96% 26860/28000 [3:23:13<08:37,  2.20it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  96% 26880/28000 [3:23:16<08:28,  2.20it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  96% 26900/28000 [3:23:19<08:18,  2.20it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  96% 26920/28000 [3:23:23<08:09,  2.21it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  96% 26940/28000 [3:23:26<08:00,  2.21it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  96% 26960/28000 [3:23:29<07:50,  2.21it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  96% 26980/28000 [3:23:32<07:41,  2.21it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  96% 27000/28000 [3:23:35<07:32,  2.21it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  96% 27020/28000 [3:23:39<07:23,  2.21it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  97% 27040/28000 [3:23:42<07:13,  2.21it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  97% 27060/28000 [3:23:45<07:04,  2.21it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  97% 27080/28000 [3:23:48<06:55,  2.21it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  97% 27100/28000 [3:23:51<06:46,  2.22it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  97% 27120/28000 [3:23:54<06:37,  2.22it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  97% 27140/28000 [3:23:58<06:27,  2.22it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  97% 27160/28000 [3:24:01<06:18,  2.22it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  97% 27180/28000 [3:24:04<06:09,  2.22it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  97% 27200/28000 [3:24:07<06:00,  2.22it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  97% 27220/28000 [3:24:10<05:51,  2.22it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  97% 27240/28000 [3:24:13<05:41,  2.22it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  97% 27260/28000 [3:24:17<05:32,  2.22it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  97% 27280/28000 [3:24:20<05:23,  2.23it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  98% 27300/28000 [3:24:23<05:14,  2.23it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  98% 27320/28000 [3:24:26<05:05,  2.23it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  98% 27340/28000 [3:24:29<04:56,  2.23it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  98% 27360/28000 [3:24:32<04:47,  2.23it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  98% 27380/28000 [3:24:36<04:37,  2.23it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  98% 27400/28000 [3:24:39<04:28,  2.23it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  98% 27420/28000 [3:24:42<04:19,  2.23it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  98% 27440/28000 [3:24:45<04:10,  2.23it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  98% 27460/28000 [3:24:48<04:01,  2.23it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  98% 27480/28000 [3:24:51<03:52,  2.24it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  98% 27500/28000 [3:24:55<03:43,  2.24it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  98% 27520/28000 [3:24:58<03:34,  2.24it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  98% 27540/28000 [3:25:01<03:25,  2.24it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  98% 27560/28000 [3:25:04<03:16,  2.24it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  98% 27580/28000 [3:25:07<03:07,  2.24it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  99% 27600/28000 [3:25:11<02:58,  2.24it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  99% 27620/28000 [3:25:14<02:49,  2.24it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  99% 27640/28000 [3:25:17<02:40,  2.24it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  99% 27660/28000 [3:25:20<02:31,  2.25it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  99% 27680/28000 [3:25:23<02:22,  2.25it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  99% 27700/28000 [3:25:26<02:13,  2.25it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  99% 27720/28000 [3:25:30<02:04,  2.25it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  99% 27740/28000 [3:25:33<01:55,  2.25it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  99% 27760/28000 [3:25:36<01:46,  2.25it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  99% 27780/28000 [3:25:39<01:37,  2.25it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  99% 27800/28000 [3:25:42<01:28,  2.25it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  99% 27820/28000 [3:25:45<01:19,  2.25it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4:  99% 27840/28000 [3:25:49<01:10,  2.25it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4: 100% 27860/28000 [3:25:52<01:02,  2.26it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4: 100% 27880/28000 [3:25:55<00:53,  2.26it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4: 100% 27900/28000 [3:25:58<00:44,  2.26it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4: 100% 27920/28000 [3:26:01<00:35,  2.26it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4: 100% 27940/28000 [3:26:04<00:26,  2.26it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4: 100% 27960/28000 [3:26:08<00:17,  2.26it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4: 100% 27980/28000 [3:26:11<00:08,  2.26it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4: 100% 28000/28000 [3:26:14<00:00,  2.26it/s, loss=1.76, v_num=3, val_loss=1.930, train_loss=1.810]\n","Epoch 4: 100% 28000/28000 [3:26:17<00:00,  2.26it/s, loss=1.76, v_num=3, val_loss=1.910, train_loss=1.740]\n","                                                   \u001b[AEpoch 4, global step 125999: val_loss reached 1.90701 (best 1.90701), saving model to \"/content/drive/My Drive/munning_rachine/KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=04-val_loss=1.907.ckpt\" as top 5\n","INFO:lightning:Epoch 4, global step 125999: val_loss reached 1.90701 (best 1.90701), saving model to \"/content/drive/My Drive/munning_rachine/KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=04-val_loss=1.907.ckpt\" as top 5\n","Epoch 4: 100% 28000/28000 [3:26:31<00:00,  2.26it/s, loss=1.76, v_num=3, val_loss=1.910, train_loss=1.740]Saving latest checkpoint...\n","INFO:lightning:Saving latest checkpoint...\n","Epoch 4: 100% 28000/28000 [3:26:35<00:00,  2.26it/s, loss=1.76, v_num=3, val_loss=1.910, train_loss=1.740]\n","17:23:03\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xs-Fm95p3Mg","executionInfo":{"elapsed":1061863,"status":"ok","timestamp":1633689322850,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"e283f1d1-abf8-458a-e092-2b3f5ff9f1b8"},"source":["#train model에 추가 학습 필요할 시 사용할 코드\n","!python KoBART/train.py \\\n","--train_file='dataset/korean_dialogue/Training/csv/korean_dialogue_train.csv' \\\n","--test_file='dataset/korean_dialogue/Training/csv/korean_dialogue_test.csv' \\\n","--mode='train' \\\n","--checkpoint_path='KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=01-val_loss=2.001.ckpt' \\\n","--hparams_file='KoBART/model/korean_dialogue/train/tb_logs/default/version_5/hparams.yaml' \\\n","--batch_size=10 \\\n","--num_workers=2 \\\n","--gradient_clip_val=1.0 \\\n","--gpus=1 \\\n","--accelerator=ddp \\\n","--max_epochs=1 \\\n","--default_root_dir='KoBART/model/korean_dialogue/train'"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:root:Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=10, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=01-val_loss=2.001.ckpt', default_root_dir='KoBART/model/korean_dialogue/train', deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=1.0, hparams_file='KoBART/model/korean_dialogue/train/tb_logs/default/version_5/hparams.yaml', limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=1, max_len=512, max_steps=None, min_epochs=None, min_steps=None, mode='train', model_path=None, move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test_file='dataset/korean_dialogue/Training/csv/korean_dialogue_test.csv', tpu_cores=<function _gpus_arg_default at 0x7fdfd58350e0>, track_grad_norm=-1, train_file='dataset/korean_dialogue/Training/csv/korean_dialogue_train.csv', truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n","using cached model\n","GPU available: True, used: True\n","INFO:lightning:GPU available: True, used: True\n","TPU available: None, using: 0 TPU cores\n","INFO:lightning:TPU available: None, using: 0 TPU cores\n","using cached model\n","using cached model\n","INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:root:number of workers 2, data length 252000\n","INFO:root:num_train_steps : 63000\n","INFO:root:num_warmup_steps : 6300\n","initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n","INFO:lightning:initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n","\n","  | Name  | Type                         | Params\n","-------------------------------------------------------\n","0 | model | BartForConditionalGeneration | 123 M \n","-------------------------------------------------------\n","123 M     Trainable params\n","0         Non-trainable params\n","123 M     Total params\n","495.440   Total estimated model params size (MB)\n","INFO:lightning:\n","  | Name  | Type                         | Params\n","-------------------------------------------------------\n","0 | model | BartForConditionalGeneration | 123 M \n","-------------------------------------------------------\n","123 M     Trainable params\n","0         Non-trainable params\n","123 M     Total params\n","495.440   Total estimated model params size (MB)\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0% 0/28000 [00:00<?, ?it/s] INFO:root:Reducer buckets have been rebuilt in this iteration.\n","Epoch 0:  90% 25220/28000 [3:20:27<22:05,  2.10it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Validating: 0it [00:00, ?it/s]\u001b[A\n","Validating:   0% 0/2800 [00:00<?, ?it/s]\u001b[A\n","Epoch 0:  90% 25240/28000 [3:20:30<21:55,  2.10it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  90% 25260/28000 [3:20:34<21:45,  2.10it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  90% 25280/28000 [3:20:37<21:35,  2.10it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  90% 25300/28000 [3:20:40<21:24,  2.10it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  90% 25320/28000 [3:20:43<21:14,  2.10it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  90% 25340/28000 [3:20:46<21:04,  2.10it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  91% 25360/28000 [3:20:49<20:54,  2.10it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  91% 25380/28000 [3:20:53<20:44,  2.11it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  91% 25400/28000 [3:20:56<20:34,  2.11it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  91% 25420/28000 [3:20:59<20:23,  2.11it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  91% 25440/28000 [3:21:02<20:13,  2.11it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  91% 25460/28000 [3:21:05<20:03,  2.11it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  91% 25480/28000 [3:21:08<19:53,  2.11it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  91% 25500/28000 [3:21:11<19:43,  2.11it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  91% 25520/28000 [3:21:15<19:33,  2.11it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  91% 25540/28000 [3:21:18<19:23,  2.11it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  91% 25560/28000 [3:21:21<19:13,  2.12it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  91% 25580/28000 [3:21:24<19:03,  2.12it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  91% 25600/28000 [3:21:27<18:53,  2.12it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  92% 25620/28000 [3:21:30<18:43,  2.12it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  92% 25640/28000 [3:21:34<18:33,  2.12it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  92% 25660/28000 [3:21:37<18:23,  2.12it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  92% 25680/28000 [3:21:40<18:13,  2.12it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  92% 25700/28000 [3:21:43<18:03,  2.12it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  92% 25720/28000 [3:21:46<17:53,  2.12it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  92% 25740/28000 [3:21:49<17:43,  2.13it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  92% 25760/28000 [3:21:53<17:33,  2.13it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  92% 25780/28000 [3:21:56<17:23,  2.13it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  92% 25800/28000 [3:21:59<17:13,  2.13it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  92% 25820/28000 [3:22:02<17:03,  2.13it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  92% 25840/28000 [3:22:05<16:53,  2.13it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  92% 25860/28000 [3:22:08<16:43,  2.13it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  92% 25880/28000 [3:22:12<16:33,  2.13it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  92% 25900/28000 [3:22:15<16:23,  2.13it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  93% 25920/28000 [3:22:18<16:14,  2.14it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  93% 25940/28000 [3:22:21<16:04,  2.14it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  93% 25960/28000 [3:22:24<15:54,  2.14it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  93% 25980/28000 [3:22:27<15:44,  2.14it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  93% 26000/28000 [3:22:31<15:34,  2.14it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  93% 26020/28000 [3:22:34<15:24,  2.14it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  93% 26040/28000 [3:22:37<15:15,  2.14it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  93% 26060/28000 [3:22:40<15:05,  2.14it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  93% 26080/28000 [3:22:43<14:55,  2.14it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  93% 26100/28000 [3:22:46<14:45,  2.15it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  93% 26120/28000 [3:22:50<14:35,  2.15it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  93% 26140/28000 [3:22:53<14:26,  2.15it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  93% 26160/28000 [3:22:56<14:16,  2.15it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  94% 26180/28000 [3:22:59<14:06,  2.15it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  94% 26200/28000 [3:23:02<13:56,  2.15it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  94% 26220/28000 [3:23:05<13:47,  2.15it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  94% 26240/28000 [3:23:08<13:37,  2.15it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  94% 26260/28000 [3:23:12<13:27,  2.15it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  94% 26280/28000 [3:23:15<13:18,  2.15it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  94% 26300/28000 [3:23:18<13:08,  2.16it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  94% 26320/28000 [3:23:21<12:58,  2.16it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  94% 26340/28000 [3:23:24<12:49,  2.16it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  94% 26360/28000 [3:23:27<12:39,  2.16it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  94% 26380/28000 [3:23:31<12:29,  2.16it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  94% 26400/28000 [3:23:34<12:20,  2.16it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  94% 26420/28000 [3:23:37<12:10,  2.16it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  94% 26440/28000 [3:23:40<12:01,  2.16it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  94% 26460/28000 [3:23:43<11:51,  2.16it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  95% 26480/28000 [3:23:46<11:41,  2.17it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  95% 26500/28000 [3:23:50<11:32,  2.17it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  95% 26520/28000 [3:23:53<11:22,  2.17it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  95% 26540/28000 [3:23:56<11:13,  2.17it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  95% 26560/28000 [3:23:59<11:03,  2.17it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  95% 26580/28000 [3:24:02<10:54,  2.17it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  95% 26600/28000 [3:24:05<10:44,  2.17it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  95% 26620/28000 [3:24:09<10:35,  2.17it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  95% 26640/28000 [3:24:12<10:25,  2.17it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  95% 26660/28000 [3:24:15<10:15,  2.18it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  95% 26680/28000 [3:24:18<10:06,  2.18it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  95% 26700/28000 [3:24:21<09:57,  2.18it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  95% 26720/28000 [3:24:24<09:47,  2.18it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  96% 26740/28000 [3:24:28<09:38,  2.18it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  96% 26760/28000 [3:24:31<09:28,  2.18it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  96% 26780/28000 [3:24:34<09:19,  2.18it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  96% 26800/28000 [3:24:37<09:09,  2.18it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  96% 26820/28000 [3:24:40<09:00,  2.18it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  96% 26840/28000 [3:24:43<08:50,  2.18it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  96% 26860/28000 [3:24:47<08:41,  2.19it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  96% 26880/28000 [3:24:50<08:32,  2.19it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  96% 26900/28000 [3:24:53<08:22,  2.19it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  96% 26920/28000 [3:24:56<08:13,  2.19it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  96% 26940/28000 [3:24:59<08:03,  2.19it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  96% 26960/28000 [3:25:02<07:54,  2.19it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  96% 26980/28000 [3:25:06<07:45,  2.19it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  96% 27000/28000 [3:25:09<07:35,  2.19it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  96% 27020/28000 [3:25:12<07:26,  2.19it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  97% 27040/28000 [3:25:15<07:17,  2.20it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  97% 27060/28000 [3:25:18<07:07,  2.20it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  97% 27080/28000 [3:25:21<06:58,  2.20it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  97% 27100/28000 [3:25:25<06:49,  2.20it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  97% 27120/28000 [3:25:28<06:40,  2.20it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  97% 27140/28000 [3:25:31<06:30,  2.20it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  97% 27160/28000 [3:25:34<06:21,  2.20it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  97% 27180/28000 [3:25:37<06:12,  2.20it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  97% 27200/28000 [3:25:40<06:02,  2.20it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  97% 27220/28000 [3:25:44<05:53,  2.21it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  97% 27240/28000 [3:25:47<05:44,  2.21it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  97% 27260/28000 [3:25:50<05:35,  2.21it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  97% 27280/28000 [3:25:53<05:26,  2.21it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  98% 27300/28000 [3:25:56<05:16,  2.21it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  98% 27320/28000 [3:25:59<05:07,  2.21it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  98% 27340/28000 [3:26:03<04:58,  2.21it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  98% 27360/28000 [3:26:06<04:49,  2.21it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  98% 27380/28000 [3:26:09<04:40,  2.21it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  98% 27400/28000 [3:26:12<04:30,  2.21it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  98% 27420/28000 [3:26:15<04:21,  2.22it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  98% 27440/28000 [3:26:18<04:12,  2.22it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  98% 27460/28000 [3:26:22<04:03,  2.22it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  98% 27480/28000 [3:26:25<03:54,  2.22it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  98% 27500/28000 [3:26:28<03:45,  2.22it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  98% 27520/28000 [3:26:31<03:36,  2.22it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  98% 27540/28000 [3:26:34<03:27,  2.22it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  98% 27560/28000 [3:26:37<03:17,  2.22it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  98% 27580/28000 [3:26:40<03:08,  2.22it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  99% 27600/28000 [3:26:44<02:59,  2.23it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  99% 27620/28000 [3:26:47<02:50,  2.23it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  99% 27640/28000 [3:26:50<02:41,  2.23it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  99% 27660/28000 [3:26:53<02:32,  2.23it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  99% 27680/28000 [3:26:56<02:23,  2.23it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  99% 27700/28000 [3:26:59<02:14,  2.23it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  99% 27720/28000 [3:27:03<02:05,  2.23it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  99% 27740/28000 [3:27:06<01:56,  2.23it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  99% 27760/28000 [3:27:09<01:47,  2.23it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  99% 27780/28000 [3:27:12<01:38,  2.23it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  99% 27800/28000 [3:27:15<01:29,  2.24it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  99% 27820/28000 [3:27:18<01:20,  2.24it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0:  99% 27840/28000 [3:27:22<01:11,  2.24it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0: 100% 27860/28000 [3:27:25<01:02,  2.24it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0: 100% 27880/28000 [3:27:28<00:53,  2.24it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0: 100% 27900/28000 [3:27:31<00:44,  2.24it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0: 100% 27920/28000 [3:27:34<00:35,  2.24it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0: 100% 27940/28000 [3:27:37<00:26,  2.24it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0: 100% 27960/28000 [3:27:41<00:17,  2.24it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0: 100% 27980/28000 [3:27:44<00:08,  2.24it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0: 100% 28000/28000 [3:27:47<00:00,  2.25it/s, loss=1.21, v_num=6, val_loss=1.770, train_loss=0.951]\n","Epoch 0: 100% 28000/28000 [3:27:50<00:00,  2.25it/s, loss=1.21, v_num=6, val_loss=2.100, train_loss=1.220]\n","                                                   \u001b[AEpoch 0, global step 25199: val_loss reached 2.10030 (best 2.10030), saving model to \"/content/drive/My Drive/munning_rachine/KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=00-val_loss=2.100.ckpt\" as top 1\n","INFO:lightning:Epoch 0, global step 25199: val_loss reached 2.10030 (best 2.10030), saving model to \"/content/drive/My Drive/munning_rachine/KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=00-val_loss=2.100.ckpt\" as top 1\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n","  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","tcmalloc: large alloc 1231454208 bytes == 0x55b8586f6000 @  0x7fe072d02615 0x55b74ce844cc 0x55b74cf6447a 0x55b74ce8af0c 0x7fe0267c7a94 0x7fe0267c9864 0x7fe026799590 0x7fe0174fb465 0x7fe0174f79ca 0x7fe0174fc609 0x7fe02679cf2b 0x7fe026422200 0x55b74ce88098 0x55b74cefb4d9 0x55b74cef5ced 0x55b74ce88bda 0x55b74cef6915 0x55b74cef59ee 0x55b74ce88bda 0x55b74cefad00 0x55b74ce88afa 0x55b74cef6915 0x55b74cef59ee 0x55b74ce88bda 0x55b74cef6c0d 0x55b74cef59ee 0x55b74ce88bda 0x55b74cefad00 0x55b74ce88afa 0x55b74cef6c0d 0x55b74ce88afa\n","tcmalloc: large alloc 1539317760 bytes == 0x55b7f7af4000 @  0x7fe072d02615 0x55b74ce844cc 0x55b74cf6447a 0x55b74ce8af0c 0x7fe0267c7a94 0x7fe0267c9864 0x7fe026799590 0x7fe0174fb465 0x7fe0174f79ca 0x7fe0174fc609 0x7fe02679cf2b 0x7fe026422200 0x55b74ce88098 0x55b74cefb4d9 0x55b74cef5ced 0x55b74ce88bda 0x55b74cef6915 0x55b74cef59ee 0x55b74ce88bda 0x55b74cefad00 0x55b74ce88afa 0x55b74cef6915 0x55b74cef59ee 0x55b74ce88bda 0x55b74cef6c0d 0x55b74cef59ee 0x55b74ce88bda 0x55b74cefad00 0x55b74ce88afa 0x55b74cef6c0d 0x55b74ce88afa\n","tcmalloc: large alloc 1924153344 bytes == 0x55b8586f6000 @  0x7fe072d02615 0x55b74ce844cc 0x55b74cf6447a 0x55b74ce8af0c 0x7fe0267c7a94 0x7fe0267c9864 0x7fe026799590 0x7fe0174fb465 0x7fe0174f79ca 0x7fe0174fc609 0x7fe02679cf2b 0x7fe026422200 0x55b74ce88098 0x55b74cefb4d9 0x55b74cef5ced 0x55b74ce88bda 0x55b74cef6915 0x55b74cef59ee 0x55b74ce88bda 0x55b74cefad00 0x55b74ce88afa 0x55b74cef6915 0x55b74cef59ee 0x55b74ce88bda 0x55b74cef6c0d 0x55b74cef59ee 0x55b74ce88bda 0x55b74cefad00 0x55b74ce88afa 0x55b74cef6c0d 0x55b74ce88afa\n","Epoch 0: 100% 28000/28000 [3:27:59<00:00,  2.24it/s, loss=1.21, v_num=6, val_loss=2.100, train_loss=1.220]tcmalloc: large alloc 1539317760 bytes == 0x55b8586f6000 @  0x7fe072d02615 0x55b74ce844cc 0x55b74cf6447a 0x55b74ce8af0c 0x7fe0267c7a94 0x7fe0267c9864 0x7fe026799590 0x7fe0174fb465 0x7fe0174f79ca 0x7fe0174fc609 0x7fe02679cf2b 0x7fe026422200 0x55b74ce88098 0x55b74cefb4d9 0x55b74cef5ced 0x55b74ce88bda 0x55b74cef6915 0x55b74cef59ee 0x55b74ce88bda 0x55b74cefad00 0x55b74ce88afa 0x55b74cef6915 0x55b74cef59ee 0x55b74ce88bda 0x55b74cef6c0d 0x55b74cef59ee 0x55b74ce88bda 0x55b74cefad00 0x55b74ce88afa 0x55b74cef6c0d 0x55b74ce88afa\n","tcmalloc: large alloc 1924153344 bytes == 0x55b8b42f8000 @  0x7fe072d02615 0x55b74ce844cc 0x55b74cf6447a 0x55b74ce8af0c 0x7fe0267c7a94 0x7fe0267c9864 0x7fe026799590 0x7fe0174fb465 0x7fe0174f79ca 0x7fe0174fc609 0x7fe02679cf2b 0x7fe026422200 0x55b74ce88098 0x55b74cefb4d9 0x55b74cef5ced 0x55b74ce88bda 0x55b74cef6915 0x55b74cef59ee 0x55b74ce88bda 0x55b74cefad00 0x55b74ce88afa 0x55b74cef6915 0x55b74cef59ee 0x55b74ce88bda 0x55b74cef6c0d 0x55b74cef59ee 0x55b74ce88bda 0x55b74cefad00 0x55b74ce88afa 0x55b74cef6c0d 0x55b74ce88afa\n","Saving latest checkpoint...\n","INFO:lightning:Saving latest checkpoint...\n","Epoch 0: 100% 28000/28000 [3:28:07<00:00,  2.24it/s, loss=1.21, v_num=6, val_loss=2.100, train_loss=1.220]\n"]}]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"3DDTw0Jrp3Mh","outputId":"62fc081a-6da0-48a1-fdb3-ebe0585009ef"},"source":["# 체크포인트 파일 새로운 파일이 나오면 해당 파일로 변경해야함\n","# 하이퍼 파라미터 파일도 새로운 버전(version_10)이 나오면 변경해야함\n","# 원래 경로\n","# --train_file='dataset/new_train.csv' \\\n","# --test_file='dataset/new_test.csv' \\\n","\n","!python KoBART/train.py \\\n","--train_file='dataset/korean_dialogue/Training/csv/korean_dialogue_train.csv' \\\n","--test_file='dataset/korean_dialogue/Training/csv/korean_dialogue_test.csv' \\\n","--mode='test' \\\n","--checkpoint_path='KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=00-val_loss=2.015.ckpt' \\\n","--hparams_file='KoBART/model/korean_dialogue/train/tb_logs/default/version_3/hparams.yaml' \\\n","--batch_size=10 \\\n","--num_workers=2 \\\n","--gpus=1 \\\n","--accelerator=ddp \\\n","--default_root_dir='KoBART/model/korean_dialogue/test'"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:root:Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=10, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=00-val_loss=2.015.ckpt', default_root_dir='KoBART/model/korean_dialogue/test', deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=0, hparams_file='KoBART/model/korean_dialogue/train/tb_logs/default/version_3/hparams.yaml', limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=None, max_len=512, max_steps=None, min_epochs=None, min_steps=None, mode='test', model_path=None, move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test_file='dataset/korean_dialogue/Training/csv/korean_dialogue_test.csv', tpu_cores=<function _gpus_arg_default at 0x7f2a366c40e0>, track_grad_norm=-1, train_file='dataset/korean_dialogue/Training/csv/korean_dialogue_train.csv', truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n","[██████████████████████████████████████████████████]\n","GPU available: True, used: True\n","INFO:lightning:GPU available: True, used: True\n","TPU available: None, using: 0 TPU cores\n","INFO:lightning:TPU available: None, using: 0 TPU cores\n","[██████████████████████████████████████████████████]\n","using cached model\n","INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n","INFO:lightning:initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100% 2800/2800 [4:20:48<00:00,  5.75s/it]    rouge-1  ...   rouge-l\n","r  0.037829  ...  0.037178\n","p  0.062423  ...  0.061348\n","f  0.045026  ...  0.044242\n","\n","[3 rows x 3 columns]\n","Testing: 100% 2800/2800 [4:20:48<00:00,  5.59s/it]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{}\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"15DjT0X2p3Mh","executionInfo":{"elapsed":2019,"status":"ok","timestamp":1633711078468,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"151b3372-6440-4d69-ea8c-9d624ec7df57"},"source":["!python KoBART/download_binary.py"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Download config.json\n","Downloading...\n","From: https://drive.google.com/uc?id=1H13loH6dS_2c2Z21kaBtgz42QsjkdAwO\n","To: /content/drive/My Drive/munning_rachine/model/korean_dialogue/output/config.json\n","Traceback (most recent call last):\n","  File \"KoBART/download_binary.py\", line 13, in <module>\n","    gdown.download(config_url, config, quiet=False)\n","  File \"/usr/local/lib/python3.7/dist-packages/gdown/download.py\", line 90, in download\n","    f = open(tmp_file, 'wb')\n","FileNotFoundError: [Errno 2] No such file or directory: 'model/korean_dialogue/output/config.jsonerjf43tutmp'\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EBy6feOE4zo2","executionInfo":{"elapsed":540,"status":"ok","timestamp":1633360759147,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"723cf94b-d520-4397-edac-269a0792c2af"},"source":["cd .."],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/My Drive/munning_rachine\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCrvF2p9p3Mh","executionInfo":{"elapsed":17253,"status":"ok","timestamp":1633711386853,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"fe09cca8-9f18-49f0-ab76-0e6857ddd243"},"source":["!python KoBART/get_model_binary.py \\\n","--hparams='KoBART/model/korean_dialogue/train/tb_logs/default/version_6/hparams.yaml' \\\n","--model_binary='KoBART/model/korean_dialogue/train/kobart_summary-model_chp/epoch=00-val_loss=2.100.ckpt' \\\n","--output_dir='KoBART/model/korean_dialogue/output'"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["KoBART/get_model_binary.py:13: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  hparams = yaml.load(f)\n","using cached model\n","using cached model\n"]}]},{"cell_type":"markdown","metadata":{"id":"zzXVcgYJ_ynr"},"source":["### 일상대화 정량평가"]},{"cell_type":"code","metadata":{"id":"owQEvDKg_x6J"},"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xdB3chJnAIyt"},"source":["df = pd.read_csv('', encoding='UTF-8')\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ElxbwFn2o7cQ"},"source":["### streamlit"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"TSOSfYjtbBvk","executionInfo":{"elapsed":12004,"status":"ok","timestamp":1633711155168,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"e1cf8dbf-e2f8-4ff3-8edd-be6b2520855e"},"source":["!pip install streamlit --upgrade"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting streamlit\n","  Downloading streamlit-1.0.0-py2.py3-none-any.whl (8.3 MB)\n","\u001b[K     |████████████████████████████████| 8.3 MB 14.9 MB/s \n","\u001b[?25hRequirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.2)\n","Collecting watchdog\n","  Downloading watchdog-2.1.6-py3-none-manylinux2014_x86_64.whl (76 kB)\n","\u001b[K     |████████████████████████████████| 76 kB 6.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n","Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n","Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.4)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.0)\n","Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n","Collecting pydeck>=0.1.dev5\n","  Downloading pydeck-0.7.0-py2.py3-none-any.whl (4.3 MB)\n","\u001b[K     |████████████████████████████████| 4.3 MB 80.8 MB/s \n","\u001b[?25hCollecting gitpython!=3.1.19\n","  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n","\u001b[K     |████████████████████████████████| 180 kB 78.9 MB/s \n","\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.2.0)\n","Requirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n","Collecting blinker\n","  Downloading blinker-1.4.tar.gz (111 kB)\n","\u001b[K     |████████████████████████████████| 111 kB 86.4 MB/s \n","\u001b[?25hCollecting validators\n","  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n","Collecting base58\n","  Downloading base58-2.1.0-py3-none-any.whl (5.6 kB)\n","Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n","Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n","Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython!=3.1.19->streamlit) (3.7.4.3)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n","\u001b[?25hCollecting smmap<5,>=3.0.1\n","  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2018.9)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n","Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.5)\n","Collecting ipykernel>=5.1.2\n","  Downloading ipykernel-6.4.1-py3-none-any.whl (124 kB)\n","\u001b[K     |████████████████████████████████| 124 kB 62.0 MB/s \n","\u001b[?25hRequirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.1.0)\n","Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.1.3)\n","Requirement already satisfied: importlib-metadata<5 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.1)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n","Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (5.3.5)\n","Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.0.0)\n","Requirement already satisfied: argcomplete>=1.12.3 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.12.3)\n","Collecting ipython<8.0,>=7.23.1\n","  Downloading ipython-7.28.0-py3-none-any.whl (788 kB)\n","\u001b[K     |████████████████████████████████| 788 kB 54.8 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (3.6.0)\n","Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n","  Downloading prompt_toolkit-3.0.20-py3-none-any.whl (370 kB)\n","\u001b[K     |████████████████████████████████| 370 kB 69.4 MB/s \n","\u001b[?25hRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.18.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.5)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (2.6.1)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (57.4.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.4.2)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.2)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.8.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (22.3.0)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.1)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.5)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.12.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.8.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.1.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n","Building wheels for collected packages: blinker\n","  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=3eb2f0d1af42db1234915c2fddff5b7a0102ee6d03b615cfaf88201058231341\n","  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n","Successfully built blinker\n","Installing collected packages: prompt-toolkit, ipython, ipykernel, smmap, gitdb, watchdog, validators, pydeck, gitpython, blinker, base58, streamlit\n","  Attempting uninstall: prompt-toolkit\n","    Found existing installation: prompt-toolkit 1.0.18\n","    Uninstalling prompt-toolkit-1.0.18:\n","      Successfully uninstalled prompt-toolkit-1.0.18\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 5.5.0\n","    Uninstalling ipython-5.5.0:\n","      Successfully uninstalled ipython-5.5.0\n","  Attempting uninstall: ipykernel\n","    Found existing installation: ipykernel 4.10.1\n","    Uninstalling ipykernel-4.10.1:\n","      Successfully uninstalled ipykernel-4.10.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.20 which is incompatible.\n","google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.4.1 which is incompatible.\n","google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.28.0 which is incompatible.\u001b[0m\n","Successfully installed base58-2.1.0 blinker-1.4 gitdb-4.0.7 gitpython-3.1.24 ipykernel-6.4.1 ipython-7.28.0 prompt-toolkit-3.0.20 pydeck-0.7.0 smmap-4.0.0 streamlit-1.0.0 validators-0.18.2 watchdog-2.1.6\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","ipykernel","prompt_toolkit"]}}},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TGbg-vLtbAAY","executionInfo":{"elapsed":6063,"status":"ok","timestamp":1633711419438,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"0d1af5b4-e80c-4ed3-b94a-d04ebd224735"},"source":["!pip install pyngrok\n","!ngrok authtoken 1xiXwKjnUWUlFP1thbl5KYgsbhD_7cUjM8p7tymQrbNH4gHNG"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyngrok\n","  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n","\u001b[?25l\r\u001b[K     |▍                               | 10 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 37.1 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 42.0 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81 kB 15.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 92 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 163 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 184 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 204 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 215 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 225 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 235 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 245 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 266 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 276 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 286 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 296 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 307 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 317 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 337 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 358 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 368 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 378 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 389 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 399 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 409 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 419 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 430 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 440 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 450 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 460 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 471 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 481 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 491 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 501 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 512 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 522 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 532 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 542 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 552 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 563 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 573 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 583 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 593 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 604 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 614 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 624 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 634 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 645 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 655 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 665 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 675 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 686 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 696 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 706 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 716 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 727 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 737 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 745 kB 15.9 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (5.3.1)\n","Building wheels for collected packages: pyngrok\n","  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19006 sha256=9c46340a0518d2e363c4a740fb66f93f34a339de829e000a386e7650416f01e9\n","  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n","Successfully built pyngrok\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-5.1.0\n","Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"]}]},{"cell_type":"code","metadata":{"id":"rRi3gPlwl779"},"source":["from pyngrok import ngrok"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69u2XD7rmN2x","executionInfo":{"elapsed":4,"status":"ok","timestamp":1633711421071,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"f8f5bed5-ca9d-4da9-cb11-d85d7e4ff200"},"source":["!streamlit run KoBART/infer.py&>/dev/null&\n","url = ngrok.connect( addr='8501')\n","url"],"execution_count":null,"outputs":[{"data":{"text/plain":["<NgrokTunnel: \"http://1a6b-34-141-140-239.ngrok.io\" -> \"http://localhost:8501\">"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"hSENC0S3q8YO"},"source":["ngrok.kill()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"38giBEG1q8SD"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"IDNtTnpcTCLH"},"source":["### (hide) 구글 플레이스토어 데이터 전처리 하기"]},{"cell_type":"code","metadata":{"id":"jEAmX0G9gU1Q"},"source":["new = pd.read_csv('/content/drive/MyDrive/munning_rachine/dataset/data_final.csv', encoding='utf-8-sig')\n","path2 = '/content/drive/MyDrive/munning_rachine/dataset/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IzCA7TyV_Bt0"},"source":["from sklearn.model_selection import train_test_split\n","train, test = train_test_split(new, test_size=0.1, random_state=20)\n","train.to_csv(path_or_buf=path2+'new_train.csv', encoding = 'utf-8-sig', index=False)\n","test.to_csv(path_or_buf=path2+'new_test.csv', encoding = 'utf-8-sig', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":597},"id":"DrnIwesmxCKY","executionInfo":{"elapsed":443,"status":"ok","timestamp":1632646225229,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"8a3faba3-9380-47a3-85ee-f4b168487b10"},"source":["train"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>App Name</th>\n","      <th>text</th>\n","      <th>abstractive</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9729</th>\n","      <td>파이널삼국지2</td>\n","      <td>몇분마다보상을었고전설확룔도높아서진짜괜찮고3일에서4일 동안우편을방치하니까소환권을120...</td>\n","      <td>파이널삼국지2은(는) 확룔 소환 우편 보상 에서 긍정적인 평가를 받았어요. \\n파이...</td>\n","    </tr>\n","    <tr>\n","      <th>5249</th>\n","      <td>플레이투게더</td>\n","      <td>안녕하세요 플레이투게더를 재밌게 플레이하는 사람입니다 혹시 제작자님 제가 6월9일에...</td>\n","      <td>플레이투게더은(는) 청소 사람 투게더 제작자 최대한 보상 에서 긍정적인 평가를 받았...</td>\n","    </tr>\n","    <tr>\n","      <th>2821</th>\n","      <td>쿠키런: 킹덤</td>\n","      <td>안녕하세요!! 한 1주전에 깔은 사람인데요 데이터는 많이 나가지만 와이파이키고 하면...</td>\n","      <td>쿠키런: 킹덤은(는) 사람 난이도 유도 레벨 추천 보이 주전 와이파이 주심 재미 현...</td>\n","    </tr>\n","    <tr>\n","      <th>4145</th>\n","      <td>건담 슈프림 배틀</td>\n","      <td>일단 건덕이라 별 만점드립니다 근데...서버 열리지도않았는데 알림떠서 기대하고 깔았...</td>\n","      <td>건담 슈프림 배틀은(는) 시스템 깝놀 서버 투전 에서 긍정적인 평가를 받았어요. \\...</td>\n","    </tr>\n","    <tr>\n","      <th>4790</th>\n","      <td>브롤스타즈</td>\n","      <td>안녕하세요. 브롤스타즈를2년째 하고있는유저입니다 브롤스타즈는 정말재미있는게임입니다 ...</td>\n","      <td>브롤스타즈은(는) 터치 유저 단점 화면 과금 너프 문제 와이파이 애드 브롤 에서 긍...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3915</th>\n","      <td>피쉬돔 (Fishdom)</td>\n","      <td>게임 잼있어요~한단게한단게 올라가는게ㅎ 시간가는줄도 모르구해요.승부욕도 생기구요. ...</td>\n","      <td>피쉬돔 (Fishdom)은(는) 한단 승부 구해 시간 에서 긍정적인 평가를 받았어요...</td>\n","    </tr>\n","    <tr>\n","      <th>9620</th>\n","      <td>GIRL GLOBE</td>\n","      <td>게임 너무 좋아요!!! 근데 빨리 업데이트 좀.. 챕터 다 깨서 할게 없어요..ㅠ</td>\n","      <td>GIRL GLOBE은(는) 게임 업데이트 에서 긍정적인 평가를 받았어요. \\nGIR...</td>\n","    </tr>\n","    <tr>\n","      <th>7068</th>\n","      <td>지오메트리 대시 라이트 (Geometry Dash L)</td>\n","      <td>이건 광고는 별로없고 맵도 재밌어서 좋은것 같아요. 근데 4번 부활을 하게 해줘요 ...</td>\n","      <td>지오메트리 대시 라이트 (Geometry Dash L)은(는) 리뷰 부활 시작 처음...</td>\n","    </tr>\n","    <tr>\n","      <th>7391</th>\n","      <td>Ball Run 2048</td>\n","      <td>Wow</td>\n","      <td>Ball Run 2048은(는)  에서 긍정적인 평가를 받았어요. \\nBall Ru...</td>\n","    </tr>\n","    <tr>\n","      <th>4367</th>\n","      <td>FNF Mod Garcello, Tricky, Satoru - Full Battle</td>\n","      <td>1광고검나나옴 2타이밍안맞음 3화면구림</td>\n","      <td>FNF Mod Garcello, Tricky, Satoru - Full Battle...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9474 rows × 3 columns</p>\n","</div>"],"text/plain":["                                            App Name  ...                                        abstractive\n","9729                                         파이널삼국지2  ...  파이널삼국지2은(는) 확룔 소환 우편 보상 에서 긍정적인 평가를 받았어요. \\n파이...\n","5249                                          플레이투게더  ...  플레이투게더은(는) 청소 사람 투게더 제작자 최대한 보상 에서 긍정적인 평가를 받았...\n","2821                                         쿠키런: 킹덤  ...  쿠키런: 킹덤은(는) 사람 난이도 유도 레벨 추천 보이 주전 와이파이 주심 재미 현...\n","4145                                       건담 슈프림 배틀  ...  건담 슈프림 배틀은(는) 시스템 깝놀 서버 투전 에서 긍정적인 평가를 받았어요. \\...\n","4790                                           브롤스타즈  ...  브롤스타즈은(는) 터치 유저 단점 화면 과금 너프 문제 와이파이 애드 브롤 에서 긍...\n","...                                              ...  ...                                                ...\n","3915                                   피쉬돔 (Fishdom)  ...  피쉬돔 (Fishdom)은(는) 한단 승부 구해 시간 에서 긍정적인 평가를 받았어요...\n","9620                                      GIRL GLOBE  ...  GIRL GLOBE은(는) 게임 업데이트 에서 긍정적인 평가를 받았어요. \\nGIR...\n","7068                  지오메트리 대시 라이트 (Geometry Dash L)  ...  지오메트리 대시 라이트 (Geometry Dash L)은(는) 리뷰 부활 시작 처음...\n","7391                                   Ball Run 2048  ...  Ball Run 2048은(는)  에서 긍정적인 평가를 받았어요. \\nBall Ru...\n","4367  FNF Mod Garcello, Tricky, Satoru - Full Battle  ...  FNF Mod Garcello, Tricky, Satoru - Full Battle...\n","\n","[9474 rows x 3 columns]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"K5A1EWIt1tCE"},"source":["tt = pd.read_csv('./dataset/PlaystoreReviewData_new.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rCVLzC5f1_qR"},"source":["tt['Comment_cleansing'] = tt['Comment_processed'].map(lambda x: cleasing(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ckThon002Fxd"},"source":["tt.to_csv('./dataset/PlaystoreReviewData(new).csv', index=False, encoding='utf-8-sig')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wVvwI4ZOBqO4"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"-dgFogNZByr1"},"source":["### (hide) 구글 플레이스토어 모델링"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9BgSHG1-Byr2","executionInfo":{"elapsed":408,"status":"ok","timestamp":1633244446009,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"afeffd36-8950-4665-8575-944464554be5"},"source":["cd '/content/drive/MyDrive/munning_rachine'"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/munning_rachine\n"]}]},{"cell_type":"code","metadata":{"id":"-UxHHAnbByr3"},"source":["import time\n","import datetime\n","import torch\n","import gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_CcVehIByr4","executionInfo":{"elapsed":152681,"status":"ok","timestamp":1633245312354,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"18ccd36e-1c7e-4615-b3b9-ca9be263ac65"},"source":["start = time.time()\n","!python KoBART/train.py \\\n","--train_file='dataset/korean_dialogue/Training/train.csv' \\\n","--test_file='dataset/korean_dialogue/Training/test.csv' \\\n","--mode='train' \\\n","--batch_size=10 \\\n","--num_workers=2 \\\n","--gradient_clip_val=1.0 \\\n","--gpus=1 \\\n","--accelerator=ddp \\\n","--max_epochs=5 \\\n","--default_root_dir='KoBART/model/train'\n","\n","sec = time.time()-start\n","times = str(datetime.timedelta(seconds=sec)).split(\".\")\n","times = times[0]\n","print(times)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:root:Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=10, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path=None, default_root_dir='KoBART/model/korean_dialogue/train', deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=1.0, hparams_file=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=5, max_len=512, max_steps=None, min_epochs=None, min_steps=None, mode='train', model_path=None, move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test_file='dataset/korean_dialogue/Training/csv/korean_dialogue_test.csv', tpu_cores=<function _gpus_arg_default at 0x7f82560dc050>, track_grad_norm=-1, train_file='dataset/korean_dialogue/Training/csv/korean_dialogue_train.csv', truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n","using cached model\n","GPU available: True, used: True\n","INFO:lightning:GPU available: True, used: True\n","TPU available: None, using: 0 TPU cores\n","INFO:lightning:TPU available: None, using: 0 TPU cores\n","using cached model\n","using cached model\n","INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:root:number of workers 2, data length 252000\n","INFO:root:num_train_steps : 63000\n","INFO:root:num_warmup_steps : 6300\n","initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n","INFO:lightning:initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n","\n","  | Name  | Type                         | Params\n","-------------------------------------------------------\n","0 | model | BartForConditionalGeneration | 123 M \n","-------------------------------------------------------\n","123 M     Trainable params\n","0         Non-trainable params\n","123 M     Total params\n","495.440   Total estimated model params size (MB)\n","INFO:lightning:\n","  | Name  | Type                         | Params\n","-------------------------------------------------------\n","0 | model | BartForConditionalGeneration | 123 M \n","-------------------------------------------------------\n","123 M     Trainable params\n","0         Non-trainable params\n","123 M     Total params\n","495.440   Total estimated model params size (MB)\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Epoch 0:   0% 0/28000 [00:00<?, ?it/s] INFO:root:Reducer buckets have been rebuilt in this iteration.\n","Epoch 0:   1% 280/28000 [02:11<3:36:11,  2.14it/s, loss=4.33, v_num=2, val_loss=10.70, train_loss=4.040]/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n","  warnings.warn(*args, **kwargs)\n","Saving latest checkpoint...\n","INFO:lightning:Saving latest checkpoint...\n","Traceback (most recent call last):\n","  File \"KoBART/train.py\", line 355, in <module>\n","    trainer.fit(model, dm)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 513, in fit\n","    self.dispatch()\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 553, in dispatch\n","    self.accelerator.start_training(self)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\", line 74, in start_training\n","    self.training_type_plugin.start_training(trainer)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 111, in start_training\n","    self._results = trainer.run_train()\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 676, in run_train\n","    self.train_loop.on_train_end()\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 134, in on_train_end\n","    self.check_checkpoint_callback(should_update=True, is_last=True)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py\", line 164, in check_checkpoint_callback\n","    cb.on_validation_end(self.trainer, model)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 212, in on_validation_end\n","    self.save_checkpoint(trainer, pl_module)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 247, in save_checkpoint\n","    self._validate_monitor_key(trainer)\n","  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 490, in _validate_monitor_key\n","    raise MisconfigurationException(m)\n","pytorch_lightning.utilities.exceptions.MisconfigurationException: ModelCheckpoint(monitor='val_loss') not found in the returned metrics: ['train_loss']. HINT: Did you call self.log('val_loss', tensor) in the LightningModule?\n","0:02:32\n"]}]},{"cell_type":"code","metadata":{"id":"U7wcFrEGByr4"},"source":["#train model에 추가 학습 필요할 시 사용할 코드\n","!python KoBART/train.py \\\n","--train_file='dataset/new_train2.csv' \\\n","--test_file='dataset/new_test2.csv' \\\n","--mode='train' \\\n","--checkpoint_path='KoBART/model/train/kobart_summary-model_chp/epoch=01-val_loss=5.443.ckpt' \\\n","--hparams_file='KoBART/model/train/tb_logs/default/version_2/hparams.yaml' \\\n","--batch_size=10 \\\n","--num_workers=2 \\\n","--gradient_clip_val=1.0 \\\n","--gpus=1 \\\n","--accelerator=ddp \\\n","--max_epochs=5 \\\n","--default_root_dir='KoBART/model/train'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"sbnqBXQ5Byr4","outputId":"0c45cb90-cb9b-4c5a-9dde-b45f6bb9650e"},"source":["# 체크포인트 파일 새로운 파일이 나오면 해당 파일로 변경해야함\n","# 하이퍼 파라미터 파일도 새로운 버전(version_10)이 나오면 변경해야함\n","# 원래 경로\n","# --train_file='dataset/new_train.csv' \\\n","# --test_file='dataset/new_test.csv' \\\n","\n","!python KoBART/train.py \\\n","--train_file='dataset/korean_dialogue/Training/train.csv' \\\n","--test_file='dataset/korean_dialogue/Training/test.csv' \\\n","--mode='test' \\\n","--checkpoint_path='KoBART/model/train/kobart_summary-model_chp/epoch=01-val_loss=5.443.ckpt' \\\n","--hparams_file='KoBART/model/train/tb_logs/default/version_2/hparams.yaml' \\\n","--batch_size=10 \\\n","--num_workers=2 \\\n","--gpus=1 \\\n","--accelerator=ddp \\\n","--default_root_dir='KoBART/model/test'"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:root:Namespace(accelerator='ddp', accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=10, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path='KoBART/model/train/kobart_summary-model_chp/epoch=01-val_loss=5.443.ckpt', default_root_dir='KoBART/model/test', deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=0, hparams_file='KoBART/model/train/tb_logs/default/version_2/hparams.yaml', limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=None, max_len=512, max_steps=None, min_epochs=None, min_steps=None, mode='test', model_path=None, move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=2, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test_file='dataset/korean_dialogue/Training/test.csv', tpu_cores=<function _gpus_arg_default at 0x7f0c6fdce0e0>, track_grad_norm=-1, train_file='dataset/korean_dialogue/Training/train.csv', truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n","using cached model\n","GPU available: True, used: True\n","INFO:lightning:GPU available: True, used: True\n","TPU available: None, using: 0 TPU cores\n","INFO:lightning:TPU available: None, using: 0 TPU cores\n","using cached model\n","using cached model\n","INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n","INFO:lightning:initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  warnings.warn(*args, **kwargs)\n","Testing: 100% 1529/1529 [5:54:18<00:00, 13.88s/it]    rouge-1  ...   rouge-l\n","r  0.005615  ...  0.005609\n","p  0.083832  ...  0.083799\n","f  0.010326  ...  0.010316\n","\n","[3 rows x 3 columns]\n","Testing: 100% 1529/1529 [5:54:18<00:00, 13.90s/it]\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{}\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EgOtzLJ2Byr6","executionInfo":{"elapsed":127678,"status":"ok","timestamp":1632958009563,"user":{"displayName":"rachine munning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9kuEB3YlvPYiMMogXlAuWUEghvsd5vJgn6T-x=s64","userId":"00487687558923561330"},"user_tz":-540},"outputId":"5b9240ce-aeeb-434c-cfe4-7d1cbdf2f9d1"},"source":["!python KoBART/get_model_binary.py \\\n","--hparams='KoBART/model/train/tb_logs/default/version_2/hparams.yaml' \\\n","--model_binary='KoBART/model/train/kobart_summary-model_chp/epoch=01-val_loss=5.443.ckpt' \\\n","--output_dir='KoBART/model/output'"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["KoBART/get_model_binary.py:13: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  hparams = yaml.load(f)\n","[██████████████████████████████████████████████████]\n","[██████████████████████████████████████████████████]\n"]}]}]}